{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKQ4hqN16FWScva/Nq4w5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1lZAFH8Wpic","outputId":"f94b2af9-86a5-4469-eabd-ed3efaafc1bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sktime\n","  Downloading sktime-0.36.1-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.4.2)\n","Requirement already satisfied: numpy<2.3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime) (24.2)\n","Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.2)\n","Collecting scikit-base<0.13.0,>=0.6.1 (from sktime)\n","  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: scikit-learn<1.7.0,>=0.24 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.6.1)\n","Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.14.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=0.24->sktime) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n","Downloading sktime-0.36.1-py3-none-any.whl (37.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.0/37.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m677.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-base, sktime\n","Successfully installed scikit-base-0.12.2 sktime-0.36.1\n"]}],"source":["# Import same packages as the train script in Wave-U-Net-Pytorch\n","import argparse\n","import os\n","import time\n","from functools import partial\n","\n","import torch\n","import pickle\n","import numpy as np\n","\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim import Adam\n","from torch.nn import L1Loss\n","from tqdm import tqdm\n","from torchsummary import summary\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","# install torchaudio if not already installed\n","# ! pip install torchaudio\n","import torchaudio\n","\n","import matplotlib.pyplot as plt\n","from typing import Tuple, List, Dict, Optional\n","\n","!pip install sktime\n","from sktime.transformations.panel.rocket import MiniRocketMultivariate\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/git_projects/spring_2025_dl_audio_project"]},{"cell_type":"code","source":["class MusdbDataset(Dataset):\n","\n","  def __init__(self, musDB, steps = 256):\n","    self.mel_specs = torch.zeros(1, 2, 128, steps)\n","    self.sample_rates = torch.tensor([0])\n","\n","    print(\"Tracks in MusDB:\", len(musDB))\n","\n","    for track in musDB:\n","      stems, rate = track.stems, track.rate\n","\n","      # separate the vocal from other instruments and conver to mono signal\n","      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n","      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n","\n","      # compute log mel spectrogram and convert to pytorch tensor\n","      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n","      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n","\n","      num_slices = logmelspec_novocal.shape[1] // steps\n","\n","      # chop off the last bit so that number of stft steps is a multiple of step size\n","      logmelspec_novocal = logmelspec_novocal[0:128 , 0:num_slices*steps]\n","      logmelspec_vocal = logmelspec_vocal[0:128, 0:num_slices*steps]\n","\n","      logmelspec_novocal = torch.reshape(logmelspec_novocal, (num_slices, 128, steps))\n","      logmelspec_vocal = torch.reshape(logmelspec_vocal, (num_slices, 128, steps))\n","\n","      # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n","      logmels = torch.cat((logmelspec_novocal.unsqueeze(1), logmelspec_vocal.unsqueeze(1)), 1)\n","      self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n","      self.sample_rates = torch.cat((self.sample_rates, torch.Tensor([rate])), 0)\n","\n","    # remove the all zeros slice that we initialized with\n","    self.mel_specs = self.mel_specs[1: , : , : , :]\n","    self.sample_rates = self.sample_rates[1:]\n","\n","  def __len__(self):\n","    return self.mel_specs.shape[0]\n","\n","  def __getitem__(self, ndx):\n","    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n","    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n","\n","  def _mel_spectrogram(self, audio, rate):\n","    # compute the log-mel-spectrogram of the audio at the given sample rate\n","    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n","\n","\n","class LibriSpeechDataset(Dataset):\n","\n","  def __init__(self, path, steps = 256, num_specs = 7647):\n","    self.mel_specs = self.mel_specs = torch.zeros(1, 128, steps)\n","    self.sample_rates = torch.tensor([0])\n","\n","    num_files_opened = 0\n","\n","    for speaker_dir in os.listdir(path):\n","      speaker_path = path + \"/\" + speaker_dir\n","      for chapter_dir in os.listdir(speaker_path):\n","        chapter_path = speaker_path + \"/\" + chapter_dir\n","        for file in os.listdir(chapter_path):\n","          # checks file extension and stops when we hit desired number of spectrograms (num_specs)\n","          if file.endswith('.flac') and self.mel_specs.shape[0] - 1 < num_specs:\n","\n","            try:\n","              # get audio file and convert to log mel spectrogram\n","              speech, rate = librosa.load(chapter_path + \"/\" + file, sr = 44100)\n","              mel_spec = torch.from_numpy(self._mel_spectrogram(speech, rate))\n","\n","              # Saves the total number of 128 x (steps) spectrograms\n","              num_slices = mel_spec.shape[1] // steps\n","\n","              # chop off the last bit so that number of stft steps is a multiple of step size\n","              mel_spec = mel_spec[ : , 0 : num_slices*steps]\n","\n","              # reshape the tensor to have many spectrograms of size 128 x (steps)\n","              mel_spec = torch.transpose(torch.reshape(mel_spec, (128, num_slices, steps)), 0, 1)\n","\n","              # concatenate tensor to the full tensor in the Dataset object\n","              self.mel_specs = torch.cat((self.mel_specs, mel_spec), 0)\n","              self.sample_rates = torch.cat((self.sample_rates, torch.Tensor([rate])), 0)\n","              num_files_opened += 1\n","\n","            except:\n","              print(\"failed to open \" + file)\n","\n","\n","    # chop off the zero layer we initialized with\n","    self.mel_specs = self.mel_specs[1:]\n","    self.sample_rates = self.sample_rates[1:]\n","    print(\"opened \" + str(num_files_opened) + \" files\")\n","    print(\"collected \" + str(self.mel_specs.shape[0]) + \" chunks\")\n","\n","  def __len__(self):\n","    return self.mel_specs.shape[0]\n","\n","  def __getitem__(self, ndx):\n","    return self.mel_specs[ndx], self.sample_rates[ndx]\n","\n","  def _mel_spectrogram(self, audio, rate):\n","    # compute the log-mel-spectrogram of the audio at the given sample rate\n","    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))"],"metadata":{"id":"yXytAHulXDDV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/git_projects/spring_2025_dl_audio_project_data/\"\n","\n","# The string below is the path to the saved MusdbDataset in your Drive\n","musdbDataset_path = path + \"musdb18_DatasetObject.pt\"\n","\n","# The string below is the path to the saved LibriSpeechDataset in your Drive\n","librispeechDataset_path = path + \"LibriSpeechDatasetObject.pt\"\n","\n","musdb_dataset = torch.load(musdbDataset_path, weights_only=False)\n","librispeech_dataset = torch.load(librispeechDataset_path, weights_only=False)\n","\n"],"metadata":{"id":"758jcnpdXDFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This fixes the problem with the sample rates\n","musdb_dataset.sample_rates = torch.full((len(musdb_dataset),), 44100)\n","librispeech_dataset.sample_rates = torch.full((len(musdb_dataset),), 44100)\n","\n","# Because of the way the librispeech dataset was constructed, it is slightly longer\n","# than the musbd dataset. Crop the librispeech dataset with these lines\n","librispeech_dataset.mel_specs = librispeech_dataset.mel_specs[0:len(musdb_dataset)]\n","librispeech_dataset.sample_rates = librispeech_dataset.sample_rates[0:len(musdb_dataset)]"],"metadata":{"id":"-igA3kQ7XDID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lhzKjWeMXDNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hd1wi77oXDPj"},"execution_count":null,"outputs":[]}]}