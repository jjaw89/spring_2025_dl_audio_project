{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/LibriSpeechDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kyXOOgSzMGH",
        "outputId": "4cc8ad61-ec70-456c-c31d-d74698983f83"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "%cd '/content/drive/MyDrive/DeepLearningBootcamp/LibriSpeech/dev-clean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JWZGSAOzwa4"
      },
      "outputs": [],
      "source": [
        "class LibriSpeechDataset(Dataset):\n",
        "\n",
        "  def __init__(self, path, steps = 256, num_specs = 7647):\n",
        "    self.mel_specs = self.mel_specs = torch.zeros(1, 128, steps)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_files_opened = 0\n",
        "\n",
        "    for speaker_dir in os.listdir(path):\n",
        "      speaker_path = path + \"/\" + speaker_dir\n",
        "      for chapter_dir in os.listdir(speaker_path):\n",
        "        chapter_path = speaker_path + \"/\" + chapter_dir\n",
        "        for file in os.listdir(chapter_path):\n",
        "          # checks file extension and stops when we hit desired number of spectrograms (num_specs)\n",
        "          if file.endswith('.flac') and self.mel_specs.shape[0] - 1 < num_specs:\n",
        "\n",
        "            try:\n",
        "              # get audio file and convert to log mel spectrogram\n",
        "              speech, rate = librosa.load(chapter_path + \"/\" + file, sr = 44100)\n",
        "              mel_spec = torch.from_numpy(self._mel_spectrogram(speech, rate))\n",
        "\n",
        "              # Saves the total number of 128 x (steps) spectrograms\n",
        "              num_slices = mel_spec.shape[1] // steps\n",
        "\n",
        "              # chop off the last bit so that number of stft steps is a multiple of step size\n",
        "              mel_spec = mel_spec[ : , 0 : num_slices*steps]\n",
        "\n",
        "              # reshape the tensor to have many spectrograms of size 128 x (steps)\n",
        "              mel_spec = torch.transpose(torch.reshape(mel_spec, (128, num_slices, steps)), 0, 1)\n",
        "\n",
        "              # concatenate tensor to the full tensor in the Dataset object\n",
        "              self.mel_specs = torch.cat((self.mel_specs, mel_spec), 0)\n",
        "              self.sample_rates = torch.cat((self.sample_rates, torch.Tensor([rate])), 0)\n",
        "              num_files_opened += 1\n",
        "\n",
        "            except:\n",
        "              print(\"failed to open \" + file)\n",
        "\n",
        "\n",
        "    # chop off the zero layer we initialized with\n",
        "    self.mel_specs = self.mel_specs[1:]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "    print(\"opened \" + str(num_files_opened) + \" files\")\n",
        "    print(\"collected \" + str(self.mel_specs.shape[0]) + \" chunks\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    return self.mel_specs[ndx], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J1qJCErWtoXa",
        "outputId": "931e3eb6-daac-47eb-ffd9-503d94fcc6b7"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/DeepLearningBootcamp/LibriSpeech/dev-clean\"\n",
        "data = LibriSpeechDataset(path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMtioc7ZI/KQzECE6pidJvM",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
