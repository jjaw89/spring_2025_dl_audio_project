{"cells":[{"cell_type":"markdown","metadata":{"id":"4qxrWZHYxV36"},"source":["Here we train our first version of the GAN.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vARIOK_HxV36"},"source":["## Initialize Wave-U-Net\n","\n","We start by loading the necessary packages\n","\n","Wave-U-Net is named ``generator``"]},{"cell_type":"code","source":["# %pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n","# %pip uninstall stempeg    # musdb installs the wrong version of stempeg'"],"metadata":{"id":"yfjgCEsl31aK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_3vGK4LxV37","executionInfo":{"status":"ok","timestamp":1743435953526,"user_tz":420,"elapsed":3044,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}},"outputId":"0de83a0b-010d-43d3-b947-d8b857b16195"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sktime in /usr/local/lib/python3.11/dist-packages (0.36.0)\n","Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.4.2)\n","Requirement already satisfied: numpy<2.3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime) (24.2)\n","Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.2)\n","Requirement already satisfied: scikit-base<0.13.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (0.12.0)\n","Requirement already satisfied: scikit-learn<1.7.0,>=0.24 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.6.1)\n","Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.14.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=0.24->sktime) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/git_projects/spring_2025_dl_audio_project\n","GPU: True\n"]}],"source":["# Import same packages as the train script in Wave-U-Net-Pytorch\n","import argparse\n","import os\n","import time\n","from functools import partial\n","\n","import torch\n","import pickle\n","import numpy as np\n","\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.optim import Adam\n","from torch.nn import L1Loss\n","from tqdm import tqdm\n","from torchsummary import summary\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","# install torchaudio if not already installed\n","# ! pip install torchaudio\n","import torchaudio\n","\n","import matplotlib.pyplot as plt\n","from typing import Tuple, List, Dict, Optional\n","\n","!pip install sktime\n","from sktime.transformations.panel.rocket import MiniRocketMultivariate\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/git_projects/spring_2025_dl_audio_project\n","\n","\n","# add a path to Wave-U-Net\n","import sys\n","sys.path.append('Wave-U-Net-Pytorch')\n","\n","import model.utils as model_utils\n","import utils\n","from model.waveunet import Waveunet\n","\n","# Check to see if we have a GPU available\n","print(\"GPU:\", torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{"id":"rok6FpDgxV37"},"source":["We define the parameters of the model."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"H755Zr5RxV37","executionInfo":{"status":"ok","timestamp":1743435651885,"user_tz":420,"elapsed":10,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}}},"outputs":[],"source":["model_config = {\n","    \"num_inputs\": 256,               # 128 mel bins per spectrogram, but we have to spectrograms\n","    \"num_outputs\": 128,              # Output also has 128 mel bins\n","    \"num_channels\": [32*2, 32*4, 32*8],    # Example channel progression\n","    \"instruments\": [\"vocal\"],        # Only output vocal, so no music branch\n","    \"kernel_size\": 3,                # Must be odd\n","    \"target_output_size\": 256,       # Desired output time frames (post-processing may crop)\n","    \"conv_type\": \"normal\",           # Set to \"normal\" to meet assertion requirements\n","    \"res\": \"fixed\",                  # Use fixed resampling\n","    \"separate\": False,                # Separate branch for vocal\n","    \"depth\": 1,                      # Number of conv layers per block\n","    \"strides\": 2                   # Down/up-sampling stride\n","}"]},{"cell_type":"markdown","metadata":{"id":"dFFCD4BRxV38"},"source":["Load the model, check how much GPU memory it will use during training, and print a summary of the model."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wCkCtLVZxV38","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743435706763,"user_tz":420,"elapsed":1748,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}},"outputId":"a7da9d5e-3fc5-4b0f-de26-9a9856f02ae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using valid convolutions with 289 inputs and 257 outputs\n","Peak GPU memory allocated (bytes): 955764736\n","Current GPU memory allocated (bytes): 153949696\n","|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 150341 KiB |    911 MiB |   3091 MiB |   2945 MiB |\n","|       from large pool | 139776 KiB |    908 MiB |   3076 MiB |   2939 MiB |\n","|       from small pool |  10565 KiB |     12 MiB |     15 MiB |      5 MiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 150341 KiB |    911 MiB |   3091 MiB |   2945 MiB |\n","|       from large pool | 139776 KiB |    908 MiB |   3076 MiB |   2939 MiB |\n","|       from small pool |  10565 KiB |     12 MiB |     15 MiB |      5 MiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 150338 KiB |    908 MiB |   3078 MiB |   2931 MiB |\n","|       from large pool | 139776 KiB |    905 MiB |   3062 MiB |   2926 MiB |\n","|       from small pool |  10562 KiB |     12 MiB |     15 MiB |      5 MiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1216 MiB |   1216 MiB |   1216 MiB |      0 B   |\n","|       from large pool |   1202 MiB |   1202 MiB |   1202 MiB |      0 B   |\n","|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |   7354 KiB |  86989 KiB | 591685 KiB | 584330 KiB |\n","|       from large pool |   5632 KiB |  83520 KiB | 575483 KiB | 569851 KiB |\n","|       from small pool |   1722 KiB |   3469 KiB |  16202 KiB |  14479 KiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |      88    |     108    |     238    |     150    |\n","|       from large pool |       3    |      33    |     101    |      98    |\n","|       from small pool |      85    |     105    |     137    |      52    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |      88    |     108    |     238    |     150    |\n","|       from large pool |       3    |      33    |     101    |      98    |\n","|       from small pool |      85    |     105    |     137    |      52    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      40    |      40    |      40    |       0    |\n","|       from large pool |      33    |      33    |      33    |       0    |\n","|       from small pool |       7    |       7    |       7    |       0    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       8    |      28    |      72    |      64    |\n","|       from large pool |       3    |      23    |      60    |      57    |\n","|       from small pool |       5    |       6    |      12    |       7    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv1d-1              [-1, 64, 287]          49,216\n","         ConvLayer-2              [-1, 64, 287]               0\n","            Conv1d-3             [-1, 128, 285]          24,704\n","         ConvLayer-4             [-1, 128, 285]               0\n","        Resample1d-5             [-1, 128, 143]               0\n"," DownsamplingBlock-6  [[-1, 128, 143], [-1, 64, 287]]               0\n","            Conv1d-7             [-1, 128, 141]          49,280\n","         ConvLayer-8             [-1, 128, 141]               0\n","            Conv1d-9             [-1, 256, 139]          98,560\n","        ConvLayer-10             [-1, 256, 139]               0\n","       Resample1d-11              [-1, 256, 70]               0\n","DownsamplingBlock-12  [[-1, 256, 70], [-1, 128, 141]]               0\n","           Conv1d-13              [-1, 256, 68]         196,864\n","        ConvLayer-14              [-1, 256, 68]               0\n","       Resample1d-15             [-1, 256, 135]               0\n","           Conv1d-16             [-1, 128, 133]          98,432\n","        ConvLayer-17             [-1, 128, 133]               0\n","           Conv1d-18             [-1, 128, 131]          98,432\n","        ConvLayer-19             [-1, 128, 131]               0\n","  UpsamplingBlock-20             [-1, 128, 131]               0\n","       Resample1d-21             [-1, 128, 261]               0\n","           Conv1d-22              [-1, 64, 259]          24,640\n","        ConvLayer-23              [-1, 64, 259]               0\n","           Conv1d-24              [-1, 64, 257]          24,640\n","        ConvLayer-25              [-1, 64, 257]               0\n","  UpsamplingBlock-26              [-1, 64, 257]               0\n","           Conv1d-27             [-1, 128, 257]           8,320\n","================================================================\n","Total params: 673,088\n","Trainable params: 673,088\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.28\n","Forward/backward pass size (MB): 5028.32\n","Params size (MB): 2.57\n","Estimated Total Size (MB): 5031.17\n","----------------------------------------------------------------\n"]}],"source":["# Ensure that you have a CUDA-enabled device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Instantiate and move the model to GPU\n","generator = Waveunet(**model_config).to(device)\n","\n","# Set up a dummy optimizer and loss function\n","optimizer = Adam(generator.parameters(), lr=1e-3)\n","loss_fn = L1Loss()\n","\n","# Define a dummy batch size\n","batch_size = 256\n","\n","# Create a dummy input tensor with the required shape\n","# model.num_inputs corresponds to the number of channels (256 in your config)\n","# model.input_size is the computed length (353, for instance)\n","dummy_input = torch.randn(batch_size, generator.num_inputs, generator.input_size, device=device)\n","\n","# Create a dummy target tensor with the shape that your model outputs.\n","# For a single output branch (vocal), the output shape should be:\n","# (batch_size, num_outputs, model.output_size)\n","# model.num_outputs is 128 and model.output_size is computed (257 in your case)\n","dummy_target = torch.randn(batch_size, generator.num_outputs, generator.output_size, device=device)\n","\n","# Reset GPU peak memory stats\n","torch.cuda.reset_peak_memory_stats(device)\n","\n","# Run a single forward and backward pass\n","optimizer.zero_grad()\n","# If separate is False, the model returns a dictionary; pass the correct key.\n","output = generator(dummy_input)[\"vocal\"]\n","loss = loss_fn(output, dummy_target)\n","loss.backward()\n","optimizer.step()\n","\n","# Retrieve GPU memory stats\n","peak_memory = torch.cuda.max_memory_allocated(device)\n","current_memory = torch.cuda.memory_allocated(device)\n","print(\"Peak GPU memory allocated (bytes):\", peak_memory)\n","print(\"Current GPU memory allocated (bytes):\", current_memory)\n","\n","# Optionally, print a detailed memory summary\n","print(torch.cuda.memory_summary(device=device))\n","\n","\n","summary(generator, input_size=(generator.num_inputs,  generator.input_size))\n"]},{"cell_type":"markdown","metadata":{"id":"J3urQe1yxV38"},"source":["Optionally compile the model to potentially decrease training time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7srOJSNJxV38"},"outputs":[],"source":["# generator = torch.compile(generator, mode='max-autotune')"]},{"cell_type":"markdown","metadata":{"id":"Txk2TxUixV38"},"source":["If we compile the model, to save it after training, we have to uncompile it using the following code:\n","\n","```python\n","orig_generator = generator._orig_mod\n","path = \"\"\n","torch.save(orig_generator.state_dict(), path + \"generator_state_dict.pt\")\n"]},{"cell_type":"markdown","metadata":{"id":"mjCddsoTxV39"},"source":["## Initialize miniRocket\n","We start by loading the necessary packages"]},{"cell_type":"markdown","metadata":{"id":"Hg5T7Os6xV39"},"source":["### CPU Core Allocation for MiniRocketMultivariate\n","\n","- The implementation of `MiniRocketMultivariate` runs on the **CPU**.\n","- We need to decide how many cores to allocate for it.\n","- Some cores will be used by MiniRocket itself, while others are needed for data preparation (e.g., generating spectrograms).\n","- This allocation likely needs to be **tuned for optimal performance**.\n","- As a starting point, we detect the number of available cores and split them evenly.\n","- Note: We avoid using *all* available cores to leave some resources for the operating system and other background processes.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"XwscObQrxV39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743435724101,"user_tz":420,"elapsed":11,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}},"outputId":"cba2cdd0-88ad-4ec7-a345-e47cd838a277"},"outputs":[{"output_type":"stream","name":"stdout","text":["12\n"]}],"source":["import multiprocessing\n","num_cores = multiprocessing.cpu_count()\n","print(num_cores)\n","minirocket_n_jobs = num_cores // 2 - 1\n","dataloader_n_jobs = num_cores - minirocket_n_jobs - 1"]},{"cell_type":"markdown","metadata":{"id":"X6ZXiPhWxV39"},"source":["Create the MiniRocket model"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NhG7_-VlxV39","executionInfo":{"status":"ok","timestamp":1743435731684,"user_tz":420,"elapsed":29,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}}},"outputs":[],"source":["\n","# MiniRocket Discriminator using tsai library\n","class TsaiMiniRocketDiscriminator(nn.Module):\n","    def __init__(\n","        self,\n","        freq_bins=256,\n","        time_frames=256,\n","        num_kernels=10000,  # number of convolutional kernels\n","        hidden_dim=1024,    # Increased to handle larger feature dimension\n","        output_dim=1\n","    ):\n","        super(TsaiMiniRocketDiscriminator, self).__init__()\n","\n","        # This is the mini rocket transformer which extracts features\n","        self.rocket = MiniRocketMultivariate(num_kernels=num_kernels, n_jobs=minirocket_n_jobs)\n","        # tsai's miniRocketClassifier is implemented with MiniRocketMultivariate as well\n","        self.fitted = False   # fit before training\n","        self.freq_bins = freq_bins\n","        self.time_frames = time_frames\n","        self.num_kernels = num_kernels\n","\n","        # For 2D data handling - process each sample with proper dimensions\n","        self.example_input = np.zeros((1, freq_bins, time_frames))\n","\n","        feature_dim = num_kernels * 2  # For vocals + accompaniment\n","\n","        # Example feature reducing layers\n","        self.classifier = nn.Sequential(\n","            # First reduce the massive dimension to something manageable\n","            nn.Linear(feature_dim, hidden_dim),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            # Second hidden layer\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","\n","            # Final classification layer\n","            nn.Linear(hidden_dim // 2, output_dim),\n","            nn.Sigmoid()\n","        )\n","\n","    def fit_rocket(self, spectrograms):\n","        \"\"\"\n","            Fit MiniRocket with just one piece of vocal training data (not the entire training dataset)\n","        \"\"\"\n","        if not self.fitted:\n","            try:\n","                # Reshape for MiniRocket - it expects (n_instances, n_dimensions, series_length)\n","                # flatten the freq_bins dimension to create a multivariate time series\n","                batch_size = spectrograms.shape[0]\n","\n","                # Convert first to numpy for sktime processing\n","                sample_data = spectrograms.cpu().numpy()\n","\n","                # Reshape to sktime's expected format - reduce to single sample for fitting\n","                sample_data = sample_data[:1, 0]  # Take one sample, remove channel dim\n","\n","                # Fit on this sample\n","                self.rocket.fit(sample_data)\n","                self.fitted = True\n","\n","                # Test transform to get feature dimension\n","                test_transform = self.rocket.transform(sample_data)\n","                self.feature_dim = test_transform.shape[1]\n","\n","                print(f\"MiniRocket fitted. Feature dimension: {self.feature_dim}\")\n","\n","            except Exception as e:\n","                print(f\"Error fitting MiniRocket: {e}\")\n","                # Use a fallback if fitting fails\n","                self.fitted = True  # Mark as fitted to avoid repeated attempts\n","\n","    def extract_features(self, spectrogram):\n","        \"\"\"Extract MiniRocket features from a spectrogram\"\"\"\n","        try:\n","            # Ensure rocket is fitted\n","            if not self.fitted:\n","                self.fit_rocket(spectrogram)\n","\n","            # Convert to numpy for sktime\n","            spec_np = spectrogram.cpu().numpy()\n","\n","            # Remove channel dimension expected by sktime\n","            spec_np = spec_np[:, 0]  # [batch_size, freq_bins, time_frames]\n","\n","            # This step extracts features using the convolutional kernels, numbers specified by num_kernels\n","            features = self.rocket.transform(spec_np)\n","\n","            # Convert back to torch tensor\n","            features = torch.tensor(features, dtype=torch.float32).to(spectrogram.device)\n","\n","            return features\n","\n","        except Exception as e:\n","            print(f\"Error in feature extraction: {e}\")\n","            # Return zeros as fallback\n","            return torch.zeros((spectrogram.shape[0], self.num_kernels),\n","                              device=spectrogram.device)\n","\n","    def forward(self, vocals, accompaniment):\n","        \"\"\"\n","        Forward pass of the discriminator\n","\n","        Args:\n","            vocals: Spectrograms of shape [batch_size, channels, freq_bins, time_frames]\n","            accompaniment: Spectrograms of shape [batch_size, channels, freq_bins, time_frames]\n","        \"\"\"\n","        # Extract features from both spectrograms\n","        vocal_features = self.extract_features(vocals)\n","        accomp_features = self.extract_features(accompaniment)\n","\n","        # Concatenate features (conditional GAN)\n","        combined_features = torch.cat([vocal_features, accomp_features], dim=1)\n","\n","        # Classify as real/fake\n","        validity = self.classifier(combined_features)\n","\n","        return validity\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"b6ACeYdNxV39","executionInfo":{"status":"ok","timestamp":1743435734656,"user_tz":420,"elapsed":172,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}}},"outputs":[],"source":["discriminator = TsaiMiniRocketDiscriminator()\n","# We probably do not need to compile the model"]},{"cell_type":"markdown","metadata":{"id":"mSzq-EfaxV39"},"source":["## Load data\n","Q: Does this only load the musdb18 dataset?\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"A7HJ20m-xV39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743436825087,"user_tz":420,"elapsed":14,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}},"outputId":"4c1977b3-7744-458f-a92b-4bcd395cbecd"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/git_projects/stempeg\n"]}],"source":["# when you install musdb, pip automatically installs a version of stempeg that\n","# contains a small bug. To work around this, download the stempeg folder from\n","# the github to your drive.\n","\n","\n","\n","# The path below should be changed to the location of the stempeg package in\n","# your Drive\n","%cd '/content/drive/My Drive/git_projects/stempeg'\n","\n","\n","import stempeg\n","import musdb\n","\n","import librosa\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"bpu-Q2zGxV39","executionInfo":{"status":"ok","timestamp":1743436944256,"user_tz":420,"elapsed":16,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}}},"outputs":[],"source":["class MusdbDataset(Dataset):\n","\n","  def __init__(self, musDB, steps = 256):\n","    self.mel_specs = torch.zeros(1, 2, 128, steps)\n","    self.sample_rates = torch.tensor([0])\n","\n","    print(\"Tracks in MusDB:\", len(musDB))\n","\n","    for track in musDB:\n","      stems, rate = track.stems, track.rate\n","\n","      # separate the vocal from other instruments and conver to mono signal\n","      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n","      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n","\n","      # compute log mel spectrogram and convert to pytorch tensor\n","      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n","      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n","\n","      num_slices = logmelspec_novocal.shape[1] // steps\n","\n","      # chop off the last bit so that number of stft steps is a multiple of step size\n","      logmelspec_novocal = logmelspec_novocal[0:128 , 0:num_slices*steps]\n","      logmelspec_vocal = logmelspec_vocal[0:128, 0:num_slices*steps]\n","\n","      logmelspec_novocal = torch.reshape(logmelspec_novocal, (num_slices, 128, steps))\n","      logmelspec_vocal = torch.reshape(logmelspec_vocal, (num_slices, 128, steps))\n","\n","      # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n","      logmels = torch.cat((logmelspec_novocal.unsqueeze(1), logmelspec_vocal.unsqueeze(1)), 1)\n","      self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n","      self.sample_rates = torch.cat((self.sample_rates, torch.Tensor([rate])), 0)\n","\n","    # remove the all zeros slice that we initialized with\n","    self.mel_specs = self.mel_specs[1: , : , : , :]\n","    self.sample_rates = self.sample_rates[1:]\n","\n","  def __len__(self):\n","    return self.mel_specs.shape[0]\n","\n","  def __getitem__(self, ndx):\n","    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n","    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n","\n","  def _mel_spectrogram(self, audio, rate):\n","    # compute the log-mel-spectrogram of the audio at the given sample rate\n","    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"QwOyK0emxV39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1743437528775,"user_tz":420,"elapsed":128043,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}},"outputId":"a2398d78-5340-41e1-ea2b-c9045fcabb30"},"outputs":[{"output_type":"stream","name":"stdout","text":["['train', 'test']\n","['A Classic Education - NightOwl.stem.mp4', 'ANiMAL - Clinic A.stem.mp4', 'ANiMAL - Easy Tiger.stem.mp4', 'ANiMAL - Rockshow.stem.mp4', \"Actions - Devil's Words.stem.mp4\", 'Actions - One Minute Smile.stem.mp4', 'Actions - South Of The Water.stem.mp4', 'Aimee Norwich - Child.stem.mp4', 'Alexander Ross - Goodbye Bolero.stem.mp4', 'Alexander Ross - Velvet Curtain.stem.mp4', 'Angela Thomas Wade - Milk Cow Blues.stem.mp4', 'Atlantis Bound - It Was My Fault For Waiting.stem.mp4', 'Auctioneer - Our Future Faces.stem.mp4', 'AvaLuna - Waterduct.stem.mp4', 'BigTroubles - Phantom.stem.mp4', 'Bill Chudziak - Children Of No-one.stem.mp4', 'Black Bloc - If You Want Success.stem.mp4', 'Celestial Shore - Die For Us.stem.mp4', 'Chris Durban - Celebrate.stem.mp4', 'Clara Berry And Wooldog - Air Traffic.stem.mp4', 'Clara Berry And Wooldog - Stella.stem.mp4', 'Clara Berry And Wooldog - Waltz For My Victims.stem.mp4', 'Cnoc An Tursa - Bannockburn.stem.mp4', 'Creepoid - OldTree.stem.mp4', 'Dark Ride - Burning Bridges.stem.mp4', 'Dreamers Of The Ghetto - Heavy Love.stem.mp4', 'Drumtracks - Ghost Bitch.stem.mp4', 'Faces On Film - Waiting For Ga.stem.mp4', 'Fergessen - Back From The Start.stem.mp4', 'Fergessen - Nos Palpitants.stem.mp4', 'Fergessen - The Wind.stem.mp4', 'Flags - 54.stem.mp4', 'Giselle - Moss.stem.mp4', 'Grants - PunchDrunk.stem.mp4', 'Helado Negro - Mitad Del Mundo.stem.mp4', 'Hezekiah Jones - Borrowed Heart.stem.mp4', 'Hollow Ground - Left Blind.stem.mp4', 'Hop Along - Sister Cities.stem.mp4', 'Invisible Familiars - Disturbing Wildlife.stem.mp4', 'James May - All Souls Moon.stem.mp4', 'James May - Dont Let Go.stem.mp4', 'James May - If You Say.stem.mp4', 'James May - On The Line.stem.mp4', 'Jay Menon - Through My Eyes.stem.mp4', 'Johnny Lokke - Promises & Lies.stem.mp4', 'Johnny Lokke - Whisper To A Scream.stem.mp4', 'Jokers, Jacks & Kings - Sea Of Leaves.stem.mp4', 'Leaf - Come Around.stem.mp4', 'Leaf - Summerghost.stem.mp4', 'Leaf - Wicked.stem.mp4', 'Lushlife - Toynbee Suite.stem.mp4', 'Matthew Entwistle - Dont You Ever.stem.mp4', 'Meaxic - Take A Step.stem.mp4', 'Meaxic - You Listen.stem.mp4', 'Music Delta - 80s Rock.stem.mp4', 'Music Delta - Beatles.stem.mp4', 'Music Delta - Britpop.stem.mp4', 'Music Delta - Country1.stem.mp4', 'Music Delta - Country2.stem.mp4', 'Music Delta - Disco.stem.mp4', 'Music Delta - Gospel.stem.mp4', 'Music Delta - Grunge.stem.mp4', 'Music Delta - Hendrix.stem.mp4', 'Music Delta - Punk.stem.mp4', 'Music Delta - Reggae.stem.mp4', 'Music Delta - Rock.stem.mp4', 'Music Delta - Rockabilly.stem.mp4', 'Night Panther - Fire.stem.mp4', 'North To Alaska - All The Same.stem.mp4', 'Patrick Talbot - A Reason To Leave.stem.mp4', 'Patrick Talbot - Set Me Free.stem.mp4', \"Phre The Eon - Everybody's Falling Apart.stem.mp4\", 'Port St Willow - Stay Even.stem.mp4', 'Remember December - C U Next Time.stem.mp4', 'Secret Mountains - High Horse.stem.mp4', 'Skelpolu - Human Mistakes.stem.mp4', 'Skelpolu - Together Alone.stem.mp4', 'Snowmine - Curfews.stem.mp4', \"Spike Mullings - Mike's Sulking.stem.mp4\", 'St Vitus - Word Gets Around.stem.mp4', 'Steven Clark - Bounty.stem.mp4', 'Strand Of Oaks - Spacestation.stem.mp4', 'Sweet Lights - You Let Me Down.stem.mp4', 'Swinging Steaks - Lost My Way.stem.mp4', 'The Districts - Vermont.stem.mp4', 'The Long Wait - Back Home To Blue.stem.mp4', 'The Scarlet Brand - Les Fleurs Du Mal.stem.mp4', 'The So So Glos - Emergency.stem.mp4', \"The Wrong'Uns - Rothko.stem.mp4\", 'Tim Taler - Stalker.stem.mp4', 'Titanium - Haunted Age.stem.mp4', 'Traffic Experiment - Once More (With Feeling).stem.mp4', 'Traffic Experiment - Sirens.stem.mp4', 'Triviul - Angelsaint.stem.mp4', 'Triviul - Dorothy.stem.mp4', 'Voelund - Comfort Lives In Belief.stem.mp4', 'Wall Of Death - Femme.stem.mp4', 'Young Griffo - Blood To Bone.stem.mp4', 'Young Griffo - Facade.stem.mp4', 'Young Griffo - Pennies.stem.mp4']\n","Tracks in MusDB: 10\n","816\n"]}],"source":["# get the full data set into the workspace\n","import os\n","# print(os.listdir(\"/content/drive/My Drive/git_projects/spring_2025_dl_audio_project_data/musdb18/\"))\n","# print(os.listdir(\"/content/drive/My Drive/git_projects/spring_2025_dl_audio_project_data/musdb18/train\"))\n","\n","# %cd '/content/drive/My Drive/git_projects/spring_2025_dl_audio_project_data'\n","music = musdb.DB(\"/content/drive/My Drive/git_projects/spring_2025_dl_audio_project_data/musdb18/\", subsets=\"train\")\n","# # create a dataset out of the first 10 tracks, see how many slices of audio we have\n","data = MusdbDataset(music[0:10])\n","\n","print(len(data))\n"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"MJvxPdpxxV3-","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1743437589100,"user_tz":420,"elapsed":58135,"user":{"displayName":"Jaspar Wiart","userId":"09145650281217596223"}},"outputId":"e58b5bc7-d731-4ac3-e4af-6f2bd29cf272"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tracks in MusDB: 10\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-aebb64332cb3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# # create a dataset out of the first 10 tracks, see how many slices of audio we have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMusdbDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-99b390bde692>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, musDB, steps)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# compute log mel spectrogram and convert to pytorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mlogmelspec_novocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_novocal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mlogmelspec_vocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_vocal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-99b390bde692>\u001b[0m in \u001b[0;36m_mel_spectrogram\u001b[0;34m(self, audio, rate)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# compute the log-mel-spectrogram of the audio at the given sample rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2133\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Mel-frequency spectrogram'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2134\u001b[0m     \"\"\"\n\u001b[0;32m-> 2135\u001b[0;31m     S, n_fft = _spectrogram(\n\u001b[0m\u001b[1;32m   2136\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m         \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[1;32m   2943\u001b[0m         S = (\n\u001b[1;32m   2944\u001b[0m             np.abs(\n\u001b[0;32m-> 2945\u001b[0;31m                 stft(\n\u001b[0m\u001b[1;32m   2946\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m                     \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mbl_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbl_s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         stft_matrix[..., bl_s + off_start : bl_t + off_start] = fft.rfft(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mfft_window\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbl_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbl_t\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/fft/_backend.py\u001b[0m in \u001b[0;36m__ua_function__\u001b[0;34m(method, args, kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0m__ua_domain__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"numpy.scipy.fft\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ua_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["music[0]\n","\n","# # create a dataset out of the first 10 tracks, see how many slices of audio we have\n","data = MusdbDataset(music[0:10])\n","print(len(data))\n"]},{"cell_type":"markdown","metadata":{"id":"EbhppUQ4xV3-"},"source":["### Load LibriSpeech\n","To be completed..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LanBfUEjxV3-"},"outputs":[],"source":["# To do\n"]},{"cell_type":"markdown","metadata":{"id":"4jHUSbn2xV3-"},"source":["## Traning Loop\n","to be completed..\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQ5lLYl0xV3-"},"outputs":[],"source":["def train():\n","    pass\n"]},{"cell_type":"markdown","metadata":{"id":"2jg9I_4qxV3-"},"source":["## Train the GAN\n","The models are ``generator`` and ``discriminator``."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbLNj0VUxV3-"},"outputs":[],"source":["# To do"]},{"cell_type":"markdown","metadata":{"id":"dWffnPNExV3-"},"source":["## Save the models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6dDyxJmxV3-"},"outputs":[],"source":["# Assuming we have compiled the generator\n","orig_generator = generator._orig_mod\n","path = \"\"\n","torch.save(orig_generator.state_dict(), path + \"generator_state_dict.pt\")\n","# Save the discriminator state dict\n","torch.save(discriminator.state_dict(), path + \"discriminator_state_dict.pt\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}