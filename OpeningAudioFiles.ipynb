{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/OpeningAudioFiles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upz7C_oQFNxv",
        "outputId": "9322ceec-b140-4b37-b877-67ab56da0525"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# when you install musdb, pip automatically installs a version of stempeg that\n",
        "# contains a small bug. To work around this, download the stempeg folder from\n",
        "# the github to your drive.\n",
        "\n",
        "%pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n",
        "%pip uninstall stempeg    # musdb installs the wrong version of stempeg'\n",
        "%pip install librosa      # a package for processing audioYYY\n",
        "\n",
        "# The path below should be changed to the location of the stempeg package in\n",
        "# your Drive\n",
        "%cd '/content/drive/MyDrive/DeepLearningBootcamp'\n",
        "\n",
        "import stempeg\n",
        "import musdb\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgtnpzznJiYo"
      },
      "source": [
        "Now the we have the relevant packages downloaded, we will load in the MUSDB18 data from Google Drive. The musdb package provides a convenient data structure called DB to hold and work with the stems. The cell below should take a bit to run since we are loading in so much audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yk9CdfUJs8x",
        "outputId": "5edc7c7a-83b4-4b08-f0fc-3d6f9a3baba5"
      },
      "outputs": [],
      "source": [
        "# change the string below to the location of the musdb data in your drive\n",
        "musdb_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data\"\n",
        "music = musdb.DB(musdb_path, subsets=\"train\")\n",
        "\n",
        "# If everything has been loaded correctly, there should be 100 tracks\n",
        "print(len(music))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbMXH_RDMGkF"
      },
      "source": [
        "Next, we take a random song from the music DB and print its wave form. For each song, the music DB contains a numpy tensor of shape (5, length of song, number of channels). [Number of channels is 1 for mono audio and 2 for stereo.] The first indicates which instrument is in that audio file, and all files follow the convention:\n",
        "\n",
        "0 - whole song\n",
        "\n",
        "1 - bass\n",
        "\n",
        "2 - drums\n",
        "\n",
        "3 - other instruments\n",
        "\n",
        "4 - vocals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "MIXNKL-mMvEz",
        "outputId": "2e6cb8c7-8ece-4cc6-f517-6313fc67822a"
      },
      "outputs": [],
      "source": [
        "# choose an arbitrary track from the dataset, and record its sample rate\n",
        "stems = music[8].stems\n",
        "rate = music[8].rate\n",
        "\n",
        "whole_song = stems[0]\n",
        "bass = stems[1]\n",
        "drums = stems[2]\n",
        "other_instruments = stems[3]\n",
        "vocals = stems[4]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "# this prints the waveform (time domain) of the song\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveshow(whole_song[:,1], sr = rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IPAx2ylOzRR"
      },
      "source": [
        "Next, we print some spectrograms. A spectrogram is a 2-dimensional display of the frequency information over time. The spectrogram is the attribute of a song that seems to be analyzed most often in ml audio applications.\n",
        "\n",
        "To find the spectrogram, we cut the waveform into small time chunks and compute the fourier transform of each chunk (then take the absolute value of the resulting complex numbers). This gives a time series of the frequency information. This process is called the Short Time Fourier Transform (STFT). The librosa package implements this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "d9mbAKxEPrM9",
        "outputId": "e922b9c8-b42f-4e76-98bd-ae3b854a02ee"
      },
      "outputs": [],
      "source": [
        "whole_song_stft = librosa.stft(whole_song[:,1])\n",
        "whole_song_spectrogram = librosa.amplitude_to_db(abs(whole_song_stft))\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(whole_song_spectrogram, sr=rate, x_axis='time', y_axis='hz')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUbh_IxMQolF"
      },
      "source": [
        "Let's print the spectrogram for the bass track as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "XylT54ilQv51",
        "outputId": "7e1a6396-b439-4f06-a79a-697cc57a6171"
      },
      "outputs": [],
      "source": [
        "bass_stft = librosa.stft(bass[:,1])\n",
        "bass_spectrogram = librosa.amplitude_to_db(abs(bass_stft))\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(bass_spectrogram, sr=rate, x_axis='time', y_axis='hz')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhCEF86vRpD2"
      },
      "source": [
        "Below, we will load in some of the LibriSpeech data using librosa and convert that audio to spectrogram as well. I downloaded the \"clean development set\" from the LibriSpeech website (https://www.openslr.org/12), and saved it in my Google Drive. Files in this dataset are saved as flac files which librosa can handle directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "akdPi8QsR8mI",
        "outputId": "082cdfe4-e9ff-4c2f-a5dd-dd76f6666151"
      },
      "outputs": [],
      "source": [
        "# the sr = None argument tells load not to assume any sample rate and use the same rate from the track\n",
        "speech, rate = librosa.load('/content/drive/MyDrive/DeepLearningBootcamp/LibriSpeech/dev-clean/1673/143396/1673-143396-0000.flac', sr = None)\n",
        "\n",
        "speech_stft = librosa.stft(speech)\n",
        "speech_spectrogram = librosa.amplitude_to_db(abs(speech_stft))\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.specshow(speech_spectrogram, sr = rate, x_axis='time', y_axis='hz')\n",
        "plt.colorbar()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM/dvHGZiJ265GTJe+tCPfa",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
