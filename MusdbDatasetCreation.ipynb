{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/MusdbDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oHxPG08sHd",
        "outputId": "ed34f502-3d45-4c7b-b2cd-d8d0e8f91097"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# when you install musdb, pip automatically installs a version of stempeg that\n",
        "# contains a small bug. To work around this, download the stempeg folder from\n",
        "# the github to your drive.\n",
        "\n",
        "# %pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n",
        "# %pip uninstall stempeg    # musdb installs the wrong version of stempeg'\n",
        "\n",
        "# The path below should be changed to the location of the stempeg package in\n",
        "# your Drive\n",
        "# %cd '/content/drive/MyDrive/DeepLearningBootcamp'\n",
        "\n",
        "# import stempeg\n",
        "import musdb\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pA7TDxaWpaF"
      },
      "outputs": [],
      "source": [
        "############## ONLY RUN THIS CELL IF YOU NEED TO DOWNLOAD DATA #################\n",
        "import requests\n",
        "\n",
        "file_url = \"https://zenodo.org/records/1117372/files/musdb18.zip\"\n",
        "zip_path = \"/workspace/hdd_project_data/musdb18.zip\"\n",
        "destination_path = \"/workspace/hdd_project_data/musdb18_data\"\n",
        "\n",
        "r = requests.get(file_url, stream = True)\n",
        "with open(zip_path, \"wb\") as file:\n",
        "  for block in r.iter_content(chunk_size = 1024):\n",
        "    if block:\n",
        "      file.write(block)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/workspace/hdd_project_data/musdb18_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Gfxp1J_sLT"
      },
      "outputs": [],
      "source": [
        "class MusdbDataset(Dataset):\n",
        "\n",
        "  def __init__(self, musDB, window_size = 256, step_size = 128):\n",
        "    self.mel_specs = torch.zeros(1, 2, 128, window_size)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_songs = 0\n",
        "\n",
        "    for track in musDB:\n",
        "      stems, rate = track.stems, track.rate\n",
        "\n",
        "      num_songs += 1\n",
        "\n",
        "      # separate the vocal from other instruments and conver to mono signal\n",
        "      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n",
        "      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n",
        "\n",
        "      # compute log mel spectrogram and convert to pytorch tensor\n",
        "      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n",
        "      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n",
        "\n",
        "      start_ndx = 0\n",
        "\n",
        "      for step in range(window_size // step_size):\n",
        "        cropped_logmelspec_novocal = logmelspec_novocal[:, start_ndx:]\n",
        "        cropped_logmelspec_vocal = logmelspec_vocal[:, start_ndx:]\n",
        "        num_slices = cropped_logmelspec_novocal.shape[1] // window_size\n",
        "\n",
        "        # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "        cropped_logmelspec_novocal = cropped_logmelspec_novocal[: , 0:num_slices*window_size]\n",
        "        cropped_logmelspec_vocal = cropped_logmelspec_vocal[:, 0:num_slices*window_size]\n",
        "\n",
        "        # reshape tensors into chunks of size 128x(window_size)\n",
        "        # first dimension is number of chunks\n",
        "        cropped_logmelspec_novocal = torch.transpose(torch.reshape(cropped_logmelspec_novocal, (128, num_slices, window_size)), 0, 1)\n",
        "        cropped_logmelspec_vocal = torch.transpose(torch.reshape(cropped_logmelspec_vocal, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "        # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n",
        "        logmels = torch.cat((cropped_logmelspec_novocal.unsqueeze(1), cropped_logmelspec_vocal.unsqueeze(1)), 1)\n",
        "        self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n",
        "        self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "        if num_songs % 5 == 0:\n",
        "          print(str(num_songs) + \" songs processed; produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "    # remove the all zeros slice that we initialized with\n",
        "    self.mel_specs = self.mel_specs[1: , : , : , :]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n",
        "    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n",
        "\n",
        "  def cat(self, other_ds):\n",
        "    self.mel_specs = torch.cat((self.mel_specs, other_ds.mel_specs), 0)\n",
        "    self.sample_rates = torch.cat((self.sample_rates, other_ds.sample_rates), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woM0vis0AyST"
      },
      "outputs": [],
      "source": [
        "# get the full data set into the workspace\n",
        "music_train = musdb.DB(\"/workspace/hdd_project_data/musdb18_data\", subsets=\"train\")\n",
        "music_test = musdb.DB(\"/workspace/hdd_project_data/musdb18_data\", subsets=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Ol8Ev7XknV",
        "outputId": "fab2a720-7741-40a6-b477-7ce67e7d0cd2"
      },
      "outputs": [],
      "source": [
        "# create a dataset out of all 100 tracks, see how many slices of audio we have\n",
        "musdbData_train = MusdbDataset(music_train, step_size = 128)\n",
        "musdbData_test = MusdbDataset(music_test, step_size = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8HqLKjwLl2l"
      },
      "outputs": [],
      "source": [
        "torch.save(musdbData_train, '/workspace/hdd_project_data/musdb_withOverlap_train.pt')\n",
        "torch.save(musdbData_test, '/workspace/hdd_project_data/musdb_withOverlap_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOLwO7hp9YTOEDwoQTiBWHJ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
