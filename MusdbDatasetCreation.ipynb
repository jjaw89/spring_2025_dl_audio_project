{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/MusdbDatasetCreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oHxPG08sHd",
        "outputId": "ed34f502-3d45-4c7b-b2cd-d8d0e8f91097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting musdb\n",
            "  Downloading musdb-0.4.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from musdb) (2.0.2)\n",
            "Collecting stempeg>=0.2.3 (from musdb)\n",
            "  Downloading stempeg-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyaml (from musdb)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from musdb) (4.67.1)\n",
            "Collecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->musdb) (6.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\n",
            "Downloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyaml, ffmpeg-python, stempeg, musdb\n",
            "Successfully installed ffmpeg-python-0.2.0 musdb-0.4.2 pyaml-25.1.0 stempeg-0.2.3\n",
            "Found existing installation: stempeg 0.2.3\n",
            "Uninstalling stempeg-0.2.3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/stem2files\n",
            "    /usr/local/lib/python3.11/dist-packages/stempeg-0.2.3.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/stempeg/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled stempeg-0.2.3\n",
            "/content/drive/MyDrive/DeepLearningBootcamp\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# when you install musdb, pip automatically installs a version of stempeg that\n",
        "# contains a small bug. To work around this, download the stempeg folder from\n",
        "# the github to your drive.\n",
        "\n",
        "%pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n",
        "%pip uninstall stempeg    # musdb installs the wrong version of stempeg'\n",
        "\n",
        "# The path below should be changed to the location of the stempeg package in\n",
        "# your Drive\n",
        "%cd '/content/drive/MyDrive/DeepLearningBootcamp'\n",
        "\n",
        "import stempeg\n",
        "import musdb\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############## ONLY RUN THIS CELL IF YOU NEED TO DOWNLOAD DATA #################\n",
        "import requests\n",
        "\n",
        "file_url = \"https://zenodo.org/records/1117372/files/musdb18.zip\"\n",
        "zip_path = \"/content/gdrive/My Drive/Deep Learning Bootcamp/musdb18.zip\"\n",
        "destination_path = \"/content/gdrive/My Drive/Deep Learning Bootcamp/musdb18_data\"\n",
        "\n",
        "r = requests.get(file_url, stream = True)\n",
        "with open(zip_path, \"wb\") as file:\n",
        "  for block in r.iter_content(chunk_size = 1024):\n",
        "    if block:\n",
        "      file.write(block)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/gdrive/My Drive/Deep Learning Bootcamp/musdb18_data\")"
      ],
      "metadata": {
        "id": "3pA7TDxaWpaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Gfxp1J_sLT"
      },
      "outputs": [],
      "source": [
        "class MusdbDataset(Dataset):\n",
        "\n",
        "  def __init__(self, musDB, window_size = 256, step_size = 128):\n",
        "    self.mel_specs = torch.zeros(1, 2, 128, window_size)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_songs = 0\n",
        "\n",
        "    for track in musDB:\n",
        "      stems, rate = track.stems, track.rate\n",
        "\n",
        "      num_songs += 1\n",
        "\n",
        "      # separate the vocal from other instruments and conver to mono signal\n",
        "      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n",
        "      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n",
        "\n",
        "      # compute log mel spectrogram and convert to pytorch tensor\n",
        "      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n",
        "      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n",
        "\n",
        "      start_ndx = 0\n",
        "\n",
        "      for step in range(window_size // step_size):\n",
        "        cropped_logmelspec_novocal = logmelspec_novocal[:, start_ndx:]\n",
        "        cropped_logmelspec_vocal = logmelspec_vocal[:, start_ndx:]\n",
        "        num_slices = cropped_logmelspec_novocal.shape[1] // window_size\n",
        "\n",
        "        # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "        cropped_logmelspec_novocal = cropped_logmelspec_novocal[: , 0:num_slices*window_size]\n",
        "        cropped_logmelspec_vocal = cropped_logmelspec_vocal[:, 0:num_slices*window_size]\n",
        "\n",
        "        # reshape tensors into chunks of size 128x(window_size)\n",
        "        # first dimension is number of chunks\n",
        "        cropped_logmelspec_novocal = torch.transpose(torch.reshape(cropped_logmelspec_novocal, (128, num_slices, window_size)), 0, 1)\n",
        "        cropped_logmelspec_vocal = torch.transpose(torch.reshape(cropped_logmelspec_vocal, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "        # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n",
        "        logmels = torch.cat((cropped_logmelspec_novocal.unsqueeze(1), cropped_logmelspec_vocal.unsqueeze(1)), 1)\n",
        "        self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n",
        "        self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "        if num_songs % 5 == 0:\n",
        "          print(str(num_songs) + \" songs processed; produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "    # remove the all zeros slice that we initialized with\n",
        "    self.mel_specs = self.mel_specs[1: , : , : , :]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n",
        "    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n",
        "\n",
        "  def cat(self, other_ds):\n",
        "    self.mel_specs = torch.cat((self.mel_specs, other_ds.mel_specs), 0)\n",
        "    self.sample_rates = torch.cat((self.sample_rates, other_ds.sample_rates), 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woM0vis0AyST"
      },
      "outputs": [],
      "source": [
        "# get the full data set into the workspace\n",
        "music_train = musdb.DB(\"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data\", subsets=\"train\")\n",
        "music_test = musdb.DB(\"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data\", subsets=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7Ol8Ev7XknV",
        "outputId": "fab2a720-7741-40a6-b477-7ce67e7d0cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 songs processed; produced 328 spectrograms\n",
            "10 songs processed; produced 817 spectrograms\n",
            "15 songs processed; produced 1182 spectrograms\n",
            "20 songs processed; produced 1645 spectrograms\n",
            "25 songs processed; produced 2046 spectrograms\n",
            "30 songs processed; produced 2473 spectrograms\n",
            "35 songs processed; produced 2839 spectrograms\n",
            "40 songs processed; produced 3215 spectrograms\n",
            "45 songs processed; produced 3649 spectrograms\n",
            "50 songs processed; produced 4027 spectrograms\n",
            "55 songs processed; produced 4521 spectrograms\n",
            "60 songs processed; produced 4602 spectrograms\n",
            "65 songs processed; produced 4661 spectrograms\n",
            "70 songs processed; produced 4914 spectrograms\n",
            "75 songs processed; produced 5392 spectrograms\n",
            "80 songs processed; produced 5871 spectrograms\n",
            "85 songs processed; produced 6360 spectrograms\n",
            "90 songs processed; produced 6752 spectrograms\n",
            "95 songs processed; produced 7264 spectrograms\n",
            "100 songs processed; produced 7648 spectrograms\n"
          ]
        }
      ],
      "source": [
        "# create a dataset out of all 100 tracks, see how many slices of audio we have\n",
        "musdbData_train = MusdbDataset(music_train, step_size = 128)\n",
        "musdbData_test = MusdbDataset(music_test, step_size = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8HqLKjwLl2l"
      },
      "outputs": [],
      "source": [
        "torch.save(musdbData_train, '/content/drive/MyDrive/DeepLearningBootcamp/musdb_withOverlap_train.pt')\n",
        "torch.save(musdbData_test, '/content/drive/MyDrive/DeepLearningBootcamp/musdb_withOverlap_test.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLwO7hp9YTOEDwoQTiBWHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}