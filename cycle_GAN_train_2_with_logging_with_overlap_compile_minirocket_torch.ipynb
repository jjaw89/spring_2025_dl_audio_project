{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/cycle_GAN_train_2_with_logging_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qxrWZHYxV36"
      },
      "source": [
        "Here we train our first version of the GAN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vARIOK_HxV36"
      },
      "source": [
        "## Initialize Wave-U-Net\n",
        "\n",
        "We start by loading the necessary packages\n",
        "\n",
        "Wave-U-Net is named ``generator``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZW_oS5ciVzd",
        "outputId": "42f7499f-3012-47ca-a3b5-5d128a76b46d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/git_projects/spring_2025_dl_audio_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "f_3vGK4LxV37",
        "outputId": "393eb87a-c3f8-4bff-a29e-e5bff75ddb2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: True\n"
          ]
        }
      ],
      "source": [
        "# Import same packages as the train script in Wave-U-Net-Pytorch\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn.utils import spectral_norm\n",
        "from torch.optim import Adam\n",
        "from torch.nn import L1Loss\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "# install torchaudio if not already installed\n",
        "# ! pip install torchaudio\n",
        "import torchaudio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "\n",
        "# !pip install sktime\n",
        "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
        "#\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/git_projects/spring_2025_dl_audio_project\n",
        "\n",
        "\n",
        "# add a path to Wave-U-Net\n",
        "import sys\n",
        "sys.path.append('Wave-U-Net-Pytorch')\n",
        "sys.path.append(\"workspace/hdd_project_data\")\n",
        "\n",
        "import model.utils as model_utils\n",
        "import utils\n",
        "from model.waveunet import Waveunet\n",
        "\n",
        "# Check to see if we have a GPU available\n",
        "print(\"GPU:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yfjgCEsl31aK"
      },
      "outputs": [],
      "source": [
        "# I run these commands in the terminal that you get when you pay for Colab.\n",
        "\n",
        "# %pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n",
        "# %pip uninstall stempeg    # musdb installs the wrong version of stempeg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjCddsoTxV39"
      },
      "source": [
        "## Initialize miniRocket\n",
        "We start by loading the necessary packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg5T7Os6xV39"
      },
      "source": [
        "### CPU Core Allocation for MiniRocketMultivariate\n",
        "\n",
        "- The implementation of `MiniRocketMultivariate` runs on the **CPU**.\n",
        "- We need to decide how many cores to allocate for it.\n",
        "- Some cores will be used by MiniRocket itself, while others are needed for data preparation (e.g., generating spectrograms).\n",
        "- This allocation likely needs to be **tuned for optimal performance**.\n",
        "- As a starting point, we detect the number of available cores and split them evenly.\n",
        "- Note: We avoid using *all* available cores to leave some resources for the operating system and other background processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6ZXiPhWxV39"
      },
      "source": [
        "Create the MiniRocket model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NhG7_-VlxV39"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from time import time\n",
        "\n",
        "# # MiniRocket Discriminator using tsai library\n",
        "# class TsaiMiniRocketDiscriminator(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         freq_bins=256,\n",
        "#         time_frames=256,\n",
        "#         num_kernels=10000,  # number of convolutional kernels\n",
        "#         hidden_dim=1024,    # Increased to handle larger feature dimension\n",
        "#         output_dim=1\n",
        "#     ):\n",
        "#         super(TsaiMiniRocketDiscriminator, self).__init__()\n",
        "\n",
        "#         # This is the mini rocket transformer which extracts features\n",
        "#         self.rocket = MiniRocketMultivariate(num_kernels=num_kernels, n_jobs=minirocket_n_jobs)\n",
        "#         # tsai's miniRocketClassifier is implemented with MiniRocketMultivariate as well\n",
        "#         self.fitted = False   # fit before training\n",
        "#         self.freq_bins = freq_bins\n",
        "#         self.time_frames = time_frames\n",
        "#         self.num_kernels = num_kernels\n",
        "\n",
        "#         # For 2D data handling - process each sample with proper dimensions\n",
        "#         self.example_input = np.zeros((1, freq_bins, time_frames))\n",
        "\n",
        "#         self.feature_dim = num_kernels  # For vocals + accompaniment\n",
        "\n",
        "#         # Example feature reducing layers\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             # First reduce the massive dimension to something manageable\n",
        "#             nn.Dropout(0.3),\n",
        "#             spectral_norm(nn.Linear(19992, hidden_dim)),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Dropout(0.3),\n",
        "\n",
        "#             # Second hidden layer\n",
        "#             nn.Dropout(0.3),\n",
        "#             spectral_norm(nn.Linear(hidden_dim, hidden_dim // 2)),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Dropout(0.3),\n",
        "\n",
        "#             # Final classification layer\n",
        "#             nn.Dropout(0.3),\n",
        "#             nn.Linear(hidden_dim // 2, output_dim),\n",
        "#             nn.Sigmoid()\n",
        "#             # nn.Tanh()\n",
        "#         )\n",
        "#         #         # Example feature reducing layers\n",
        "#         # self.classifier = nn.Sequential(\n",
        "#         #     # First reduce the massive dimension to something manageable\n",
        "#         #     nn.Linear(19992, hidden_dim),\n",
        "#         #     nn.LeakyReLU(0.2),\n",
        "#         #     nn.Dropout(0.3),\n",
        "\n",
        "#         #     # Second hidden layer\n",
        "#         #     nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "#         #     nn.LeakyReLU(0.2),\n",
        "#         #     nn.Dropout(0.3),\n",
        "\n",
        "#         #     # Final classification layer\n",
        "#         #     nn.Linear(hidden_dim // 2, output_dim),\n",
        "#         #     nn.Sigmoid()\n",
        "#         # )\n",
        "\n",
        "#     def fit_rocket(self, spectrograms):\n",
        "#         \"\"\"\n",
        "#             Fit MiniRocket with just one piece of vocal training data (not the entire training dataset)\n",
        "#         \"\"\"\n",
        "#         if not self.fitted:\n",
        "#             try:\n",
        "#                 # Reshape for MiniRocket - it expects (n_instances, n_dimensions, series_length)\n",
        "#                 # flatten the freq_bins dimension to create a multivariate time series\n",
        "#                 batch_size = spectrograms.shape[0]\n",
        "\n",
        "#                 # Convert first to numpy for sktime processing\n",
        "#                 sample_data = spectrograms.detach().cpu().numpy()\n",
        "#                 # print(sample_data.shape)\n",
        "#                 # Reshape to sktime's expected format - reduce to single sample for fitting\n",
        "#                 # sample_data = sample_data[:, 0]  # Take one sample, remove channel dim\n",
        "\n",
        "#                 # Fit on this sample\n",
        "#                 self.rocket.fit(sample_data)\n",
        "#                 self.fitted = True\n",
        "\n",
        "#                 # Test transform to get feature dimension\n",
        "#                 test_transform = self.rocket.transform(sample_data)\n",
        "#                 self.feature_dim = test_transform.shape[1]\n",
        "\n",
        "#                 print(f\"MiniRocket fitted. Feature dimension: {self.feature_dim}\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error fitting MiniRocket: {e}\")\n",
        "#                 # Use a fallback if fitting fails\n",
        "#                 self.fitted = True  # Mark as fitted to avoid repeated attempts\n",
        "\n",
        "#     def extract_features(self, spectrogram):\n",
        "#         \"\"\"Extract MiniRocket features from a spectrogram\"\"\"\n",
        "#         try:\n",
        "#             # Ensure rocket is fitted\n",
        "#             if not self.fitted:\n",
        "#                 self.fit_rocket(spectrogram)\n",
        "\n",
        "#             # Convert to numpy for sktime\n",
        "#             spec_np = spectrogram.detach().cpu().numpy()\n",
        "\n",
        "#             # Remove channel dimension expected by sktime\n",
        "#             # print(spec_np.shape)\n",
        "#             # spec_np = spec_np[:, 0]  # [batch_size, freq_bins, time_frames]\n",
        "#             # print(spec_np.shape)\n",
        "\n",
        "#             # This step extracts features using the convolutional kernels, numbers specified by num_kernels\n",
        "#             # print(\"1\")\n",
        "#             features = self.rocket.transform(spec_np)\n",
        "#             # print(\"2\")\n",
        "#             # Convert back to torch tensor\n",
        "#             # print(\"features:\", features.shape)\n",
        "#             # print(features.head())\n",
        "#             features_tensor = torch.tensor(features.values).to(spectrogram.device)\n",
        "#             # print(\"features:\", features.shape)\n",
        "#             # print(\"3\")\n",
        "#             return features_tensor\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error in feature extraction: {e}\")\n",
        "#             # Return zeros as fallback\n",
        "#             return torch.zeros((spectrogram.shape[0], self.num_kernels),\n",
        "#                               device=spectrogram.device)\n",
        "\n",
        "#     def forward(self, vocals, accompaniment):\n",
        "#         \"\"\"\n",
        "#         Forward pass of the discriminator\n",
        "\n",
        "#         Args:\n",
        "#             vocals: Spectrograms of shape [batch_size, channels, freq_bins, time_frames]\n",
        "#             accompaniment: Spectrograms of shape [batch_size, channels, freq_bins, time_frames]\n",
        "#         \"\"\"\n",
        "#         # Extract features from both spectrograms\n",
        "#         # start_time = time()\n",
        "#         vocal_features = self.extract_features(vocals)\n",
        "#         accomp_features = self.extract_features(accompaniment)\n",
        "#         # print(\"extract:\", time()-start_time)\n",
        "#         # Concatenate features (conditional GAN)\n",
        "#         combined_features = torch.cat([vocal_features, accomp_features], dim=1)\n",
        "#         # print(combined_features.size())\n",
        "\n",
        "#         # Classify as real/fake\n",
        "#         # print(combined_features.size())\n",
        "#         validity = self.classifier(combined_features)\n",
        "\n",
        "#         return validity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b6ACeYdNxV39"
      },
      "outputs": [],
      "source": [
        "# import tsai\n",
        "from MINIROCKET_Pytorch import MiniRocketFeatures\n",
        "class TsaiMiniRocketDiscriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        freq_bins=128,        # Number of frequency bins (input channels)\n",
        "        time_frames=256,      # Number of time frames (sequence length)\n",
        "        num_kernels=10000,    # Number of convolutional kernels\n",
        "        hidden_dim=1024,      # Hidden dimension\n",
        "        output_dim=1          # Output dimension (1 for binary classification)\n",
        "    ):\n",
        "        super(TsaiMiniRocketDiscriminator, self).__init__()\n",
        "\n",
        "        # Use tsai's PyTorch implementation of MiniRocket\n",
        "        # Properly initialize with required parameters\n",
        "        self.rocket = MiniRocketFeatures(\n",
        "            c_in=freq_bins,            # Number of input channels/features\n",
        "            seq_len=time_frames,       # Sequence length\n",
        "            num_features=num_kernels   # Number of output features\n",
        "        )\n",
        "\n",
        "        self.fitted = False\n",
        "        self.freq_bins = freq_bins\n",
        "        self.time_frames = time_frames\n",
        "        self.num_kernels = num_kernels\n",
        "\n",
        "        # The feature dimension will be determined by the rocket output\n",
        "        # For now, we estimate it based on the number of kernels\n",
        "        self.feature_dim = num_kernels * 2  # For vocals + accompaniment\n",
        "\n",
        "        # Classifier network\n",
        "        self.classifier = nn.Sequential(\n",
        "            # Using LazyLinear allows the network to determine input size on first forward pass\n",
        "            nn.LazyLinear(hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Second hidden layer\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Final classification layer\n",
        "            nn.Linear(hidden_dim // 2, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def fit_rocket(self, x):\n",
        "        \"\"\"Initialize MiniRocket parameters with sample data\"\"\"\n",
        "        if not self.fitted:\n",
        "            with torch.no_grad():\n",
        "                # MiniRocketFeatures is already initialized with c_in and seq_len\n",
        "                # We just need to do a forward pass to complete internal initializations\n",
        "                _ = self.rocket(x)\n",
        "                self.fitted = True\n",
        "                print(f\"MiniRocket fitted on device: {x.device}\")\n",
        "\n",
        "                # Test transform to get actual feature dimension\n",
        "                test_features = self.extract_features(x[:1])\n",
        "                self.feature_dim = test_features.shape[1]\n",
        "                print(f\"Feature dimension determined: {self.feature_dim}\")\n",
        "\n",
        "    def extract_features(self, spectrogram):\n",
        "        \"\"\"Extract MiniRocket features from a spectrogram\"\"\"\n",
        "        try:\n",
        "            # Ensure rocket is fitted\n",
        "            if not self.fitted:\n",
        "                self.fit_rocket(spectrogram)\n",
        "\n",
        "            # Get features using tsai's implementation\n",
        "            features = self.rocket(spectrogram)\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in feature extraction: {e}\")\n",
        "            # Return zeros as fallback\n",
        "            return torch.zeros((spectrogram.shape[0], self.num_kernels),\n",
        "                             device=spectrogram.device)\n",
        "\n",
        "    def forward(self, vocals, accompaniment):\n",
        "        \"\"\"\n",
        "        Forward pass of the discriminator\n",
        "\n",
        "        Args:\n",
        "            vocals: Spectrograms of shape [batch_size, freq_bins, time_frames]\n",
        "            accompaniment: Spectrograms of shape [batch_size, freq_bins, time_frames]\n",
        "        \"\"\"\n",
        "        # Extract features from both spectrograms\n",
        "        vocal_features = self.extract_features(vocals)\n",
        "        accomp_features = self.extract_features(accompaniment)\n",
        "\n",
        "        # Concatenate features (conditional GAN)\n",
        "        combined_features = torch.cat([vocal_features, accomp_features], dim=1)\n",
        "\n",
        "        # Classify as real/fake\n",
        "        validity = self.classifier(combined_features)\n",
        "\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEaNx9F0Hnwa"
      },
      "source": [
        "# Import Data into Session\n",
        "\n",
        "First, we run the code that defines the custom Dataset objects. The Datasets were compiled previously and saved in .pt files. In the next cell, we load those Dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bpu-Q2zGxV39"
      },
      "outputs": [],
      "source": [
        "# %cd /content/drive/My Drive/git_projects/\n",
        "# sys.path.append('/workspace/hdd_project_data/stempeg')\n",
        "# import stempeg\n",
        "import musdb\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MusdbDataset(Dataset):\n",
        "\n",
        "  def __init__(self, musDB, window_size = 256, step_size = 128):\n",
        "    self.mel_specs = torch.zeros(1, 2, 128, window_size)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_songs = 0\n",
        "\n",
        "    for track in musDB:\n",
        "      stems, rate = track.stems, track.rate\n",
        "\n",
        "      num_songs += 1\n",
        "\n",
        "      # separate the vocal from other instruments and conver to mono signal\n",
        "      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n",
        "      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n",
        "\n",
        "      # compute log mel spectrogram and convert to pytorch tensor\n",
        "      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n",
        "      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n",
        "\n",
        "      start_ndx = 0\n",
        "\n",
        "      for step in range(window_size // step_size):\n",
        "        cropped_logmelspec_novocal = logmelspec_novocal[:, start_ndx:]\n",
        "        cropped_logmelspec_vocal = logmelspec_vocal[:, start_ndx:]\n",
        "        num_slices = cropped_logmelspec_novocal.shape[1] // window_size\n",
        "\n",
        "        # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "        cropped_logmelspec_novocal = cropped_logmelspec_novocal[: , 0:num_slices*window_size]\n",
        "        cropped_logmelspec_vocal = cropped_logmelspec_vocal[:, 0:num_slices*window_size]\n",
        "\n",
        "        # reshape tensors into chunks of size 128x(window_size)\n",
        "        # first dimension is number of chunks\n",
        "        cropped_logmelspec_novocal = torch.transpose(torch.reshape(cropped_logmelspec_novocal, (128, num_slices, window_size)), 0, 1)\n",
        "        cropped_logmelspec_vocal = torch.transpose(torch.reshape(cropped_logmelspec_vocal, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "        # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n",
        "        logmels = torch.cat((cropped_logmelspec_novocal.unsqueeze(1), cropped_logmelspec_vocal.unsqueeze(1)), 1)\n",
        "        self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n",
        "        self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "        if num_songs % 5 == 0:\n",
        "          print(str(num_songs) + \" songs processed; produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "    # remove the all zeros slice that we initialized with\n",
        "    self.mel_specs = self.mel_specs[1: , : , : , :]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n",
        "    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n",
        "\n",
        "  def cat(self, other_ds):\n",
        "    self.mel_specs = torch.cat((self.mel_specs, other_ds.mel_specs), 0)\n",
        "    self.sample_rates = torch.cat((self.sample_rates, other_ds.sample_rates), 0)\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SingingDataset(Dataset):\n",
        "\n",
        "  def __init__(self, musDB, window_size = 256, step_size = 128):\n",
        "    self.mel_specs = torch.zeros(1, 128, window_size)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_songs = 0\n",
        "\n",
        "    for track in musDB:\n",
        "      stems, rate = track.stems, track.rate\n",
        "\n",
        "      num_songs += 1\n",
        "\n",
        "      # load the vocal\n",
        "      vocal = librosa.to_mono(np.transpose(stems[4]))\n",
        "\n",
        "      # compute log mel spectrogram and convert to pytorch tensor\n",
        "      mel_spec = torch.from_numpy(self._mel_spectrogram(vocal, rate))\n",
        "\n",
        "      start_ndx = 0\n",
        "      for step in range(window_size // step_size):\n",
        "        cropped_mel_spec = mel_spec[:, start_ndx:]\n",
        "        num_slices = cropped_mel_spec.shape[1] // window_size\n",
        "\n",
        "        # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "        cropped_mel_spec = cropped_mel_spec[:, 0:num_slices*window_size]\n",
        "\n",
        "        # reshape tensors into chunks of size 128x(window_size)\n",
        "        # first dimension is number of chunks\n",
        "        cropped_mel_spec = torch.transpose(torch.reshape(cropped_mel_spec, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "        # concatenate to the big tensor\n",
        "        self.mel_specs = torch.cat((self.mel_specs, cropped_mel_spec), 0)\n",
        "        self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "\n",
        "    if num_songs % 5 == 0:\n",
        "        print(str(num_songs) + \" songs processed; produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "    # remove the all zeros slice that we initialized with\n",
        "    self.mel_specs = self.mel_specs[1: , : , :]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n",
        "    return self.mel_specs[ndx], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n",
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LibriSpeechDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, window_size = 256, step_size = 128, num_specs = 7647*2):\n",
        "        self.mel_specs = self.mel_specs = torch.zeros(1, 128, window_size)\n",
        "        self.sample_rates = torch.tensor([0])\n",
        "\n",
        "        num_files_opened = 0\n",
        "\n",
        "        for speaker_dir in os.listdir(path):\n",
        "            speaker_path = path + \"/\" + speaker_dir\n",
        "            for chapter_dir in os.listdir(speaker_path):\n",
        "                chapter_path = speaker_path + \"/\" + chapter_dir\n",
        "                for file in os.listdir(chapter_path):\n",
        "                    # checks file extension and stops when we hit desired number of spectrograms (num_specs)\n",
        "                    if file.endswith('.flac') and self.mel_specs.shape[0] - 1 < num_specs:\n",
        "                        # get audio file and convert to log mel spectrogram\n",
        "                        speech, rate = librosa.load(chapter_path + \"/\" + file, sr = 44100)\n",
        "                        mel_spec = torch.from_numpy(self._mel_spectrogram(speech, rate))\n",
        "                        start_ndx = 0\n",
        "\n",
        "                        num_files_opened += 1\n",
        "\n",
        "                        for step in range(window_size // step_size):\n",
        "                            cropped_mel_spec = mel_spec[:, start_ndx:]\n",
        "\n",
        "                            # Saves the total number of 128 x (window_size) spectrograms\n",
        "                            num_slices = cropped_mel_spec.shape[1] // window_size\n",
        "\n",
        "                            # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "                            cropped_mel_spec = cropped_mel_spec[ : , 0 : num_slices*window_size]\n",
        "\n",
        "                            # reshape the tensor to have many spectrograms of size 128 x (steps)\n",
        "                            cropped_mel_spec = torch.transpose(torch.reshape(cropped_mel_spec, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "                            # concatenate tensor to the full tensor in the Dataset object\n",
        "                            self.mel_specs = torch.cat((self.mel_specs, cropped_mel_spec), 0)\n",
        "                            self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "                            # increment start_ndx\n",
        "                            start_ndx += step_size\n",
        "\n",
        "\n",
        "                        if num_files_opened % 50 == 0:\n",
        "                            print(\"opened \" + str(num_files_opened) + \" files and produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "\n",
        "        # chop off the zero layer we initialized with\n",
        "        self.mel_specs = self.mel_specs[1:]\n",
        "        self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.mel_specs.shape[0]\n",
        "\n",
        "    def __getitem__(self, ndx):\n",
        "        return self.mel_specs[ndx], self.sample_rates[ndx]\n",
        "\n",
        "    def _mel_spectrogram(self, audio, rate):\n",
        "        # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "        return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JHlmMGgzGqb-"
      },
      "outputs": [],
      "source": [
        "path = \"/workspace/hdd_project_data/\"\n",
        "\n",
        "\n",
        "# The string below is the path to the saved LibriSpeechDataset in your Drive\n",
        "librispeechDataset_path = path + \"LibriSpeechDataset_withOverlap.pt\"\n",
        "\n",
        "\n",
        "librispeech_dataset = torch.load(librispeechDataset_path, weights_only=False)\n",
        "\n",
        "\n",
        "# The string below is the path to the saved MusdbDataset in your Drive\n",
        "musdbDataset_path = path + \"musdb_withOverlap_train.pt\"\n",
        "musdb_dataset = torch.load(musdbDataset_path, weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DUHwFy2jR2JY"
      },
      "outputs": [],
      "source": [
        "# This fixes the problem with the sample rates\n",
        "musdb_dataset.sample_rates = torch.full((len(musdb_dataset),), 44100)\n",
        "librispeech_dataset.sample_rates = torch.full((len(musdb_dataset),), 44100)\n",
        "\n",
        "# Because of the way the librispeech dataset was constructed, it is slightly longer\n",
        "# than the musbd dataset. Crop the librispeech dataset with these lines\n",
        "librispeech_dataset.mel_specs = librispeech_dataset.mel_specs[0:len(musdb_dataset)]\n",
        "librispeech_dataset.sample_rates = librispeech_dataset.sample_rates[0:len(musdb_dataset)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Peiksp9NuMGX"
      },
      "source": [
        "### Explore these datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpREtcq0mYd8",
        "outputId": "9efc8a3c-9e76-49f3-bc24-434e83dcd64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MusDB Dataset Exploration ===\n",
            "Length: 11418\n",
            "mel_specs shape: torch.Size([11418, 2, 128, 256])\n",
            "sample_rates shape: torch.Size([11418])\n",
            "\n",
            "Sample 0 - Accompaniment shape: torch.Size([128, 256])\n",
            "Sample 0 - Vocal shape: torch.Size([128, 256])\n",
            "Sample 0 - Sample rate: tensor(44100)\n",
            "\n",
            "=== LibriSpeech Dataset Exploration ===\n",
            "Length: 11418\n",
            "mel_specs shape: torch.Size([11418, 128, 256])\n",
            "sample_rates shape: torch.Size([11418])\n",
            "\n",
            "Sample 0 - Speech shape: torch.Size([128, 256])\n",
            "Sample 0 - Sample rate: tensor(44100)\n"
          ]
        }
      ],
      "source": [
        "# --- Explore the Datasets ---\n",
        "print(\"=== MusDB Dataset Exploration ===\")\n",
        "print(\"Length:\", len(musdb_dataset))\n",
        "print(\"mel_specs shape:\", musdb_dataset.mel_specs.shape)\n",
        "print(\"sample_rates shape:\", musdb_dataset.sample_rates.shape)\n",
        "print()\n",
        "accompaniment, vocal, sample_rate = musdb_dataset[0]\n",
        "print(\"Sample 0 - Accompaniment shape:\", accompaniment.size())\n",
        "print(\"Sample 0 - Vocal shape:\", vocal.size())\n",
        "print(\"Sample 0 - Sample rate:\", sample_rate)\n",
        "print()\n",
        "\n",
        "print(\"=== LibriSpeech Dataset Exploration ===\")\n",
        "print(\"Length:\", len(librispeech_dataset))\n",
        "print(\"mel_specs shape:\", librispeech_dataset.mel_specs.shape)\n",
        "print(\"sample_rates shape:\", librispeech_dataset.sample_rates.shape)\n",
        "print()\n",
        "speech, sample_rate = librispeech_dataset[0]\n",
        "print(\"Sample 0 - Speech shape:\", speech.size())\n",
        "print(\"Sample 0 - Sample rate:\", sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK8WbDmTjkNH"
      },
      "source": [
        "## Dataset Helpers Explanation\n",
        "Why New Dataset Helpers?\n",
        "\n",
        "We have created new dataset helper classes (i.e., AccompanimentData, VocalData, and SpeechData) so that we can control how the data is padded and later shuffled.\n",
        "\n",
        "- **Separation of Data:**\n",
        "We separated the vocal and accompaniment data from the MusDB dataset. In our experiments, we might want to shuffle the speech data independently of the combined music data.\n",
        "\n",
        "- **Shuffling Considerations:**\n",
        "For the vocal and accompaniment data, we want to maintain their pairing so that they are shuffled in the same order. In contrast, we want the speech data to be shuffled independently.\n",
        "\n",
        "- **Future Extensions:**\n",
        "In the future, we may add another helper class that combines the vocal and accompaniment data to ensure synchronized shuffling in our data loaders.\n",
        "\n",
        "This modular approach gives us flexibility in handling and preprocessing the data for our GAN training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O7OGNnS8XMVd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AccompanimentData(Dataset):\n",
        "    def __init__(self, musdb_dataset, output_length=289):\n",
        "        self.musdb_dataset = musdb_dataset\n",
        "        self.output_length = output_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.musdb_dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        accompaniment, _, _ = self.musdb_dataset[index]  # shape: [128, 256]\n",
        "        current_len = accompaniment.size(-1)             # 256\n",
        "        delta = self.output_length - current_len         # 289 - 256 = 33\n",
        "\n",
        "        # If delta is positive, pad. Otherwise, you might want to crop or handle differently.\n",
        "        if delta > 0:\n",
        "            # Half the remainder goes to the front\n",
        "            left_pad_len = (delta // 2) + (delta % 2)  # 17\n",
        "            right_pad_len = delta // 2                # 16\n",
        "            accompaniment_pad = F.pad(accompaniment,\n",
        "                                  (left_pad_len, right_pad_len),\n",
        "                                  \"constant\", 0)\n",
        "        return {\"no_pad\" : accompaniment, \"pad\" : accompaniment_pad}\n",
        "\n",
        "\n",
        "class VocalData(Dataset):\n",
        "    def __init__(self, musdb_dataset, output_length=289):\n",
        "        self.musdb_dataset = musdb_dataset\n",
        "        self.output_length = output_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.musdb_dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        _, vocal, _ = self.musdb_dataset[index]  # shape: [128, 256]\n",
        "        current_len = vocal.size(-1)\n",
        "        delta = self.output_length - current_len\n",
        "\n",
        "        if delta > 0:\n",
        "            left_pad_len = (delta // 2) + (delta % 2)\n",
        "            right_pad_len = delta // 2\n",
        "            vocal_pad = F.pad(vocal, (left_pad_len, right_pad_len), \"constant\", 0)\n",
        "        return {\"no_pad\" : vocal, \"pad\" : vocal_pad}\n",
        "\n",
        "\n",
        "\n",
        "class SpeechData(Dataset):\n",
        "    def __init__(self, librispeech_dataset, output_length=289):\n",
        "        self.librispeech_dataset = librispeech_dataset\n",
        "        self.output_length = output_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.librispeech_dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        speech, _ = self.librispeech_dataset[index]\n",
        "        # If speech has multiple slices, pick the first slice\n",
        "        if speech.dim() == 3:\n",
        "            speech = speech[0]  # shape: [128, 256]\n",
        "        current_len = speech.size(-1)\n",
        "        delta = self.output_length - current_len\n",
        "\n",
        "        if delta > 0:\n",
        "            left_pad_len = (delta // 2) + (delta % 2)\n",
        "            right_pad_len = delta // 2\n",
        "            speech_pad = F.pad(speech, (left_pad_len, right_pad_len), \"constant\", 0)\n",
        "        return {\"no_pad\" : speech, \"pad\" : speech_pad}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gW1Twbo8lWY0"
      },
      "outputs": [],
      "source": [
        "# print(AccompanimentData(musdb_dataset)[0])\n",
        "# print(VocalData(musdb_dataset)[0])\n",
        "# print(SpeechData(librispeech_dataset)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suMDmB62yS_N"
      },
      "source": [
        "### DataLoader Explanation\n",
        "What is a DataLoader and Why Do We Need It?\n",
        "\n",
        "A DataLoader in PyTorch is a utility that wraps a dataset and provides:\n",
        "\n",
        "- **Batching:** It divides your dataset into batches so that you can train your models with mini-batch gradient descent.\n",
        "\n",
        "- **Shuffling:** It shuffles the data at every epoch (if specified) to help reduce overfitting and ensure the model sees a diverse set of examples.\n",
        "\n",
        "- **Parallel Data Loading:** It can load data in parallel using multiple worker processes, speeding up training.\n",
        "\n",
        "In our case, we create separate DataLoaders for:\n",
        "\n",
        "- The accompaniment data (paired with vocals) from the MusDB dataset.\n",
        "\n",
        "- The vocal data (paired with accompaniment) from the MusDB dataset.\n",
        "\n",
        "- The speech data from the LibriSpeech dataset.\n",
        "\n",
        "This lets us shuffle the speech data independently, while keeping the vocal/accompaniment pairs synchronized during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vLcvcILSlW3C"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Print how many batches each DataLoader contains\n",
        "# print(\"Accompaniment loader length:\", len(accompaniment_loader))\n",
        "# print(\"Vocal loader length:\", len(vocal_loader))\n",
        "# print(\"Speech loader length:\", len(speech_loader))\n",
        "\n",
        "# # Optionally, fetch and print the shape of the first batch\n",
        "# accompaniment_batch = next(iter(accompaniment_loader))\n",
        "# vocal_batch = next(iter(vocal_loader))\n",
        "# speech_batch = next(iter(speech_loader))\n",
        "# print(accompaniment_batch[\"pad\"])\n",
        "\n",
        "# print(\"Accompaniment first batch shape:\", accompaniment_batch.shape)\n",
        "# print(\"Vocal first batch shape:\", vocal_batch.shape)\n",
        "# print(\"Speech first batch shape:\", speech_batch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xUqt-HDRxKB"
      },
      "source": [
        "## Second Generator Model\n",
        "Here we initialize the second generator model whose purpose is to convert the generated vocals back to normal speach for the cycle GAN. We again use Wave-U-Net, but with a different configuration. The main difference is that we will not input the music along with the vocal track."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ql5ZsesRvvd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1FQ3ZdSymH"
      },
      "source": [
        "## Transform Input to generator_2\n",
        "\n",
        "The output of the generator model is a (batch_size, 128, 257) tensor. The model expects a tensor of size (batch_size, 128, 289). We need to pad the last dimension with 16 zeros on each size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rQn9K5nVSu-c"
      },
      "outputs": [],
      "source": [
        "def transform_for_gen_2(batch, output_length = 289):\n",
        "  current_len = batch.size(-1)\n",
        "  delta = output_length - current_len\n",
        "\n",
        "  if delta > 0:\n",
        "      left_pad_len = (delta // 2) + (delta % 2)\n",
        "      right_pad_len = delta // 2\n",
        "      batch = F.pad(batch, (left_pad_len, right_pad_len), \"constant\", 0)\n",
        "  return batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jg9I_4qxV3-"
      },
      "source": [
        "## Train the Cycle GAN\n",
        "The models are ``generator`` and ``discriminator`` and ``generator_2``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f3MBnVQYjpBB"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "def convert_to_audio(mel_spectrograms, n_fft=2048, hop_length=512, power=2.0, n_iter=32):\n",
        "    audio_files = []\n",
        "    sr = 44100\n",
        "\n",
        "    for mel_spec in mel_spectrograms:\n",
        "        # Convert Mel spectrogram back to linear spectrogram\n",
        "        # mel_s\n",
        "        mel_spec = mel_spec.detach().cpu().numpy() # Remove batch dimension\n",
        "        print(mel_spec)\n",
        "        # print(mel_spec.shape)\n",
        "        linear_spec = librosa.feature.inverse.mel_to_stft(\n",
        "            mel_spec, sr=sr, n_fft=n_fft, power=power\n",
        "        )\n",
        "\n",
        "        # Apply Griffin-Lim algorithm for phase reconstruction\n",
        "        audio = librosa.griffinlim(\n",
        "            linear_spec, hop_length=hop_length, n_iter=n_iter\n",
        "        )\n",
        "        print(audio)\n",
        "        audio_files.append(audio)\n",
        "        # break\n",
        "\n",
        "    return audio_files\n",
        "\n",
        "def display_audio(audio_file):\n",
        "    # y, sr = librosa.load(audio_file, sr=44100)\n",
        "    ipd.display(ipd.Audio(audio_file, rate=44100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A_oMQsfWSlJX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# ----- Single Epoch Training Function -----\n",
        "def train_epoch(\n",
        "    generator,\n",
        "    generator_2,\n",
        "    discriminator,\n",
        "    optimizer_D,\n",
        "    optimizer_G,\n",
        "    optimizer_G2,\n",
        "    accompaniment_loader,\n",
        "    vocal_loader,\n",
        "    speech_loader,\n",
        "    l1_loss,\n",
        "    lambda_l1,\n",
        "    lambda_cycle,\n",
        "    adversarial_loss,\n",
        "    device,\n",
        "    virtual_batch_size=1,\n",
        "    save_output=False,\n",
        "):\n",
        "    total_loss_D = total_loss_G = total_loss_G_adv = total_loss_cycle = 0.0\n",
        "    # Optionally record gradient norms per batch for diagnosing vanishing gradients.\n",
        "    grad_norms_D = []\n",
        "    grad_norms_G = []\n",
        "    num_batches = 0\n",
        "\n",
        "    # ---- batch loop ----\n",
        "    for ((accomp, voc), speech) in tqdm(\n",
        "        zip(zip(accompaniment_loader, vocal_loader), speech_loader),\n",
        "        desc=\"Training Batches\"\n",
        "    ):\n",
        "        # Move data to device\n",
        "        x_acc = accomp[\"pad\"].float().to(device)       # [B,128,289]\n",
        "        x_speech = speech[\"pad\"].float().to(device)    # [B,128,289]\n",
        "        x_in = torch.cat([x_speech, x_acc], dim=1)     # [B,256,289]\n",
        "\n",
        "        # Discriminator real/fake labels\n",
        "        B = x_acc.size(0)\n",
        "        # real_labels = torch.ones(B, 1, device=device)\n",
        "        # fake_labels = torch.zeros(B, 1, device=device)\n",
        "        \n",
        "        real_labels = torch.rand(B,1, device=device) * 0.2 + 0.8  # [0.8,1.0]\n",
        "        fake_labels = torch.rand(B,1, device=device) * 0.2        # [0.0,0.2]\n",
        "        \n",
        "        # real_labels = torch.ones(B, 1, device=device)\n",
        "        # fake_labels = -torch.ones(B, 1, device=device)\n",
        "        \n",
        "        # 1) Train D\n",
        "        optimizer_D.zero_grad()\n",
        "        acc_np = accomp[\"no_pad\"].float().to(device)\n",
        "        voc_np = voc[\"no_pad\"].float().to(device)\n",
        "\n",
        "        pred_real = discriminator(voc_np, acc_np)\n",
        "        loss_D_real = adversarial_loss(pred_real, real_labels)\n",
        "\n",
        "        raw_fake = generator(x_in)[\"vocal\"]\n",
        "        fake = raw_fake.clone()\n",
        "        fake_crop = torch.narrow(fake, 2, 0, 256).clone()\n",
        "\n",
        "        pred_fake = discriminator(fake_crop, acc_np)\n",
        "        loss_D_fake = adversarial_loss(pred_fake, fake_labels)\n",
        "\n",
        "\n",
        "\n",
        "        loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
        "        if loss_D.item() > 0.5:\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        # 2) Train G & G2\n",
        "        if virtual_batch_size == 1:\n",
        "            optimizer_G.zero_grad()\n",
        "            optimizer_G2.zero_grad()\n",
        "\n",
        "        pred_for_G = discriminator(fake, acc_np)\n",
        "        loss_G_adv = adversarial_loss(pred_for_G, real_labels)\n",
        "\n",
        "\n",
        "        # cycleâ€‘consistency\n",
        "        fake_pad = transform_for_gen_2(fake)  # you must define this\n",
        "        raw_rec = generator_2(fake_pad)[\"speech\"]\n",
        "        rec = raw_rec.clone()\n",
        "        rec_crop = torch.narrow(rec, 2, 0, 256).clone()\n",
        "        speech_np = speech[\"no_pad\"].float().to(device)\n",
        "\n",
        "        loss_cycle = l1_loss(rec_crop, speech_np)\n",
        "\n",
        "        # convex combination\n",
        "        loss_G = (loss_G_adv + lambda_cycle * loss_cycle) / (1 + lambda_cycle)\n",
        "        loss_G.backward()\n",
        "        \n",
        "        # Record discriminator gradient norms.\n",
        "        grad_norm = 0.0\n",
        "        count = 0\n",
        "        for p in discriminator.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_norm += p.grad.norm().item()\n",
        "                count += 1\n",
        "        if count > 0:\n",
        "            grad_norms_D.append(grad_norm / count)\n",
        "\n",
        "                # Record generator gradient norms.\n",
        "        grad_norm = 0.0\n",
        "        count = 0\n",
        "        \n",
        "        for p in generator.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_norm += p.grad.norm().item()\n",
        "                count += 1\n",
        "        if count > 0:\n",
        "            grad_norms_G.append(grad_norm / count)\n",
        "            \n",
        "        if (num_batches + 1) % virtual_batch_size == 0:\n",
        "            optimizer_G.step()\n",
        "            optimizer_G2.step()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_loss_D      += loss_D.item()\n",
        "        total_loss_G_adv  += loss_G_adv.item()\n",
        "        total_loss_cycle  += loss_cycle.item()\n",
        "        total_loss_G      += loss_G.item()\n",
        "        num_batches       += 1\n",
        "        # audio_files = []\n",
        "        # if save_output:\n",
        "        #     fake_singing_list = [fake_crop[i] for i in range(fake_crop.shape[0])]\n",
        "        #     audio_files = convert_to_audio(fake_singing_list)\n",
        "    return {\n",
        "        \"loss_D\":      total_loss_D / num_batches,\n",
        "        \"loss_G_adv\":  total_loss_G_adv / num_batches,\n",
        "        \"loss_cycle\":  total_loss_cycle / num_batches,\n",
        "        \"loss_G\":      total_loss_G / num_batches,\n",
        "        \"avg_grad_norm_D\": sum(grad_norms_D) / len(grad_norms_D) if grad_norms_D else 0.0,\n",
        "        \"avg_grad_norm_G\": sum(grad_norms_G) / len(grad_norms_G) if grad_norms_G else 0.0,\n",
        "    }#, audio_files\n",
        "\n",
        "# ----- Multi-Epoch Training Function -----\n",
        "def train(\n",
        "    generator,\n",
        "    generator_2,\n",
        "    discriminator,\n",
        "    optimizer_D,\n",
        "    optimizer_G,\n",
        "    optimizer_G2,\n",
        "    accompaniment_loader,\n",
        "    vocal_loader,\n",
        "    speech_loader,\n",
        "    l1_loss,\n",
        "    lambda_l1,\n",
        "    lambda_cycle,\n",
        "    adversarial_loss,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    virtual_batch_size,\n",
        "    log_dir,\n",
        "    save_audio = True\n",
        "):\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        save_audio = True if epoch == num_epochs-1 else False\n",
        "        print(f\"\\n=== Epoch {epoch+1}/{num_epochs} ===\")\n",
        "        epoch_metrics = train_epoch(\n",
        "            generator,\n",
        "            generator_2,\n",
        "            discriminator,\n",
        "            optimizer_D,\n",
        "            optimizer_G,\n",
        "            optimizer_G2,\n",
        "            accompaniment_loader,\n",
        "            vocal_loader,\n",
        "            speech_loader,\n",
        "            l1_loss,\n",
        "            lambda_l1,\n",
        "            lambda_cycle,\n",
        "            adversarial_loss,\n",
        "            device,\n",
        "            virtual_batch_size,\n",
        "            save_output = save_audio\n",
        "        )\n",
        "        print(f\"Epoch {epoch+1} Metrics:\")\n",
        "        print(f\"  Loss_D:         {epoch_metrics['loss_D']:.4f}\")\n",
        "        # print(f\"  Loss_G_total:   {epoch_metrics['loss_G_total']:.4f}\")\n",
        "        print(f\"  Loss_G_adv:     {epoch_metrics['loss_G_adv']:.4f}\")\n",
        "        # print(f\"  Loss_G_L1:      {epoch_metrics['loss_G_L1']:.4f}\")\n",
        "        print(f\"  Loss_Cycle:     {epoch_metrics['loss_cycle']:.4f}\")\n",
        "        print(f\"  Grad Norm D:    {epoch_metrics['avg_grad_norm_D']:.4f}\")\n",
        "        print(f\"  Grad Norm G:    {epoch_metrics['avg_grad_norm_G']:.4f}\")\n",
        "\n",
        "        # Log metrics to TensorBoard.\n",
        "        writer.add_scalar(\"Loss/Discriminator\", epoch_metrics[\"loss_D\"], epoch)\n",
        "        # writer.add_scalar(\"Loss/Generator_total\", epoch_metrics[\"loss_G_total\"], epoch)\n",
        "        writer.add_scalar(\"Loss/Generator_adversarial\", epoch_metrics[\"loss_G_adv\"], epoch)\n",
        "        # writer.add_scalar(\"Loss/Generator_L1\", epoch_metrics[\"loss_G_L1\"], epoch)\n",
        "        writer.add_scalar(\"Loss/Cycle\", epoch_metrics[\"loss_cycle\"], epoch)\n",
        "        writer.add_scalar(\"Gradients/Discriminator\", epoch_metrics[\"avg_grad_norm_D\"], epoch)\n",
        "        writer.add_scalar(\"Gradients/Generator\", epoch_metrics[\"avg_grad_norm_G\"], epoch)\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "    writer.close()\n",
        "    return #audio_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "phkEdf7rSckQ"
      },
      "outputs": [],
      "source": [
        "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = \"runs/\" + \"cycleGAN_experiment_\" + now\n",
        "\n",
        "# ---------------- hyperâ€‘parameters in ONE place ----------------\n",
        "train_parameters = {\n",
        "    # optimisation\n",
        "    \"lr_G\":          1e-4,\n",
        "    \"lr_G2\":         1e-4,\n",
        "    \"lr_D\":          4e-4,\n",
        "    \"betas\":         (0.9, 0.999),\n",
        "\n",
        "    # loss weights\n",
        "    \"lambda_l1\":     1,\n",
        "    \"lambda_cycle\": 0.001,\n",
        "\n",
        "\n",
        "    # schedule\n",
        "    \"num_epochs\":    20,\n",
        "    \"virtual_batch_size\": 2,\n",
        "\n",
        "    # bookkeeping\n",
        "    \"log_dir\":       f\"runs/cycleGAN_experiment_{now}\",\n",
        "    \"model_dir\":     \"models\",\n",
        "}\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 128  # Change as needed\n",
        "num_workers = 4\n",
        "# Create data loaders\n",
        "accompaniment_loader = DataLoader(\n",
        "    AccompanimentData(musdb_dataset),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "vocal_loader = DataLoader(\n",
        "    VocalData(musdb_dataset),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "speech_loader = DataLoader(\n",
        "    SpeechData(librispeech_dataset),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrrtFAj8Tjlv",
        "outputId": "8bffef33-9684-4b77-8ce4-b14a618a2b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Using valid convolutions with 289 inputs and 257 outputs\n",
            "Using valid convolutions with 289 inputs and 257 outputs\n",
            "Number of CPU cores: 24\n",
            "MiniRocket fitted on device: cuda:0\n",
            "Feature dimension determined: 9996\n"
          ]
        }
      ],
      "source": [
        "# ----- Example Setup and Training Invocation -----\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Model configurations for generator and generator_2.\n",
        "model_config_gen = {\n",
        "    \"num_inputs\": 256,  # Two spectrograms concatenated (2 * 128 mel bins)\n",
        "    \"num_outputs\": 128,\n",
        "    \"num_channels\": [512*2, 512*4, 512*8],\n",
        "    \"instruments\": [\"vocal\"],\n",
        "    \"kernel_size\": 3,\n",
        "    \"target_output_size\": 256,\n",
        "    \"conv_type\": \"normal\",\n",
        "    \"res\": \"fixed\",\n",
        "    \"separate\": False,\n",
        "    \"depth\": 1,\n",
        "    \"strides\": 2\n",
        "}\n",
        "generator = Waveunet(**model_config_gen).to(device)\n",
        "\n",
        "model_config_gen2 = {\n",
        "    \"num_inputs\": 128,  # One spectrogram input\n",
        "    \"num_outputs\": 128,\n",
        "    \"num_channels\": [256*2, 256*4, 256*8],\n",
        "    \"instruments\": [\"speech\"],\n",
        "    \"kernel_size\": 3,\n",
        "    \"target_output_size\": 256,\n",
        "    \"conv_type\": \"normal\",\n",
        "    \"res\": \"fixed\",\n",
        "    \"separate\": False,\n",
        "    \"depth\": 1,\n",
        "    \"strides\": 2\n",
        "}\n",
        "generator_2 = Waveunet(**model_config_gen2).to(device)\n",
        "# check the number of cores\n",
        "import multiprocessing\n",
        "num_cores = multiprocessing.cpu_count()\n",
        "print(\"Number of CPU cores:\", num_cores)\n",
        "# minirocket_n_jobs = num_cores-2 # Instantiate the discriminator.\n",
        "discriminator = TsaiMiniRocketDiscriminator().to(device)\n",
        "\n",
        "# Assume dataloaders `accompaniment_loader`, `vocal_loader`, and `speech_loader` are defined.\n",
        "\n",
        "# Optionally, prepare the discriminator (e.g., pre-fitting on some speech data).\n",
        "accompaniment_batch = next(iter(accompaniment_loader))\n",
        "# vocal_batch = next(iter(vocal_loader))\n",
        "speech_batch = next(iter(speech_loader))[\"no_pad\"].to(device)\n",
        "with torch.no_grad():\n",
        "    discriminator.fit_rocket(speech_batch)\n",
        "\n",
        "# Loss functions.\n",
        "adversarial_loss = nn.BCELoss().to(device)\n",
        "# adversarial_loss = nn.HingeEmbeddingLoss().to(device)\n",
        "l1_loss = nn.L1Loss().to(device)\n",
        "\n",
        "optimizer_G  = optim.Adam(generator.parameters(),  lr=train_parameters[\"lr_G\"],  betas=train_parameters[\"betas\"])\n",
        "optimizer_G2 = optim.Adam(generator_2.parameters(), lr=train_parameters[\"lr_G2\"], betas=train_parameters[\"betas\"])\n",
        "optimizer_D  = optim.Adam(discriminator.parameters(), lr=train_parameters[\"lr_D\"], betas=train_parameters[\"betas\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PWajAnHDMunt",
        "outputId": "ab0ab3f4-83fd-4d39-d8a4-134bf7fa41ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Epoch 1/20 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
            "  warnings.warn(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT __getitem__ /tmp/ipykernel_132598/2633641925.py line 37 \n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.269000 132899 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT __getitem__ /tmp/ipykernel_132598/2633641925.py line 37 \n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.269000 132900 torch/_dynamo/convert_frame.py:1233] \n",
            "Training Batches: 0it [00:00, ?it/s]W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT __getitem__ /tmp/ipykernel_132598/2633641925.py line 37 \n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.273000 132906 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT __getitem__ /tmp/ipykernel_132598/2633641925.py line 37 \n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 220, in _fn\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     cuda_rng_state = torch.cuda.get_rng_state()\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/random.py\", line 33, in get_rng_state\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     _lazy_init()\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
            "W0419 19:43:34.272000 132905 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT forward /tmp/ipykernel_132598/2551859191.py line 80 \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     super().run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1658, in CALL_FUNCTION\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, {})\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 378, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1658, in CALL_FUNCTION\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, {})\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/lazy.py\", line 170, in realize_and_forward\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return getattr(self.realize(), name)(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 914, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return variables.UserFunctionVariable(fn, source=source).call_function(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1748, in CALL_FUNCTION_KW\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tensor_variable = wrap_fx_proxy(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return _wrap_fx_proxy(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     ret_val = wrap_fake_exception(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return fn()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(make_error_message(e)).with_traceback(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return node.target(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return self.dispatch(func, types, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return self._cached_dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     output = self._dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2354, in _dispatch_impl\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     op_impl_out = op_impl(self, func, *args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 160, in dispatch_to_op_implementations_dict\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 769, in conv\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     out = func(**kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 723, in __call__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return self._op(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2325, in meta_conv\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     shape_out = calc_conv_nd_return_shape(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2253, in calc_conv_nd_return_shape\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     _formula(dims[i], padding[i], dilation[i], kernel_size[i], stride[i])\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method conv1d of type object at 0x768b99e1fec0>(*(FakeTensor(..., device='cuda:0', size=(128, 128, 256)), Parameter(FakeTensor(..., device='cuda:0', size=(10752, 1, 9)))), **{'padding': FakeTensor(..., size=(), dtype=torch.int64), 'dilation': FakeTensor(..., size=(), dtype=torch.int32), 'groups': 128}):\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] list index out of range\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] from user code:\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]    File \"/tmp/ipykernel_132598/2551859191.py\", line 89, in forward\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     vocal_features = self.extract_features(vocals)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/tmp/ipykernel_132598/2551859191.py\", line 71, in extract_features\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     features = self.rocket(spectrogram)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py\", line 75, in forward\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     C = F.conv1d(x, self.kernels, padding=padding, dilation=dilation, groups=self.c_in)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     super().run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1658, in CALL_FUNCTION\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, {})\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 378, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1658, in CALL_FUNCTION\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, {})\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/lazy.py\", line 170, in realize_and_forward\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return getattr(self.realize(), name)(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 914, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return variables.UserFunctionVariable(fn, source=source).call_function(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1748, in CALL_FUNCTION_KW\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     tensor_variable = wrap_fx_proxy(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return _wrap_fx_proxy(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     ret_val = wrap_fake_exception(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return fn()\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(make_error_message(e)).with_traceback(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return node.target(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return self.dispatch(func, types, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return self._cached_dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     output = self._dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2354, in _dispatch_impl\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     op_impl_out = op_impl(self, func, *args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 160, in dispatch_to_op_implementations_dict\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 769, in conv\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     out = func(**kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 723, in __call__\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     return self._op(*args, **kwargs)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2325, in meta_conv\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     shape_out = calc_conv_nd_return_shape(\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2253, in calc_conv_nd_return_shape\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     _formula(dims[i], padding[i], dilation[i], kernel_size[i], stride[i])\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method conv1d of type object at 0x768b99e1fec0>(*(FakeTensor(..., device='cuda:0', size=(128, 128, 256)), Parameter(FakeTensor(..., device='cuda:0', size=(10752, 1, 9)))), **{'padding': FakeTensor(..., size=(), dtype=torch.int64), 'dilation': FakeTensor(..., size=(), dtype=torch.int32), 'groups': 128}):\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] list index out of range\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] from user code:\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]    File \"/tmp/ipykernel_132598/2551859191.py\", line 89, in forward\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     vocal_features = self.extract_features(vocals)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/tmp/ipykernel_132598/2551859191.py\", line 71, in extract_features\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     features = self.rocket(spectrogram)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py\", line 75, in forward\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233]     C = F.conv1d(x, self.kernels, padding=padding, dilation=dilation, groups=self.c_in)\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0419 19:43:34.482000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT extract_features /tmp/ipykernel_132598/2551859191.py line 63 \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     super().run()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1658, in CALL_FUNCTION\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, {})\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/lazy.py\", line 170, in realize_and_forward\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return getattr(self.realize(), name)(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 914, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return variables.UserFunctionVariable(fn, source=source).call_function(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1748, in CALL_FUNCTION_KW\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     tensor_variable = wrap_fx_proxy(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return _wrap_fx_proxy(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     ret_val = wrap_fake_exception(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return fn()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(make_error_message(e)).with_traceback(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return node.target(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return self.dispatch(func, types, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return self._cached_dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     output = self._dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2354, in _dispatch_impl\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     op_impl_out = op_impl(self, func, *args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 160, in dispatch_to_op_implementations_dict\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 769, in conv\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     out = func(**kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 723, in __call__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return self._op(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2325, in meta_conv\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     shape_out = calc_conv_nd_return_shape(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2253, in calc_conv_nd_return_shape\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     _formula(dims[i], padding[i], dilation[i], kernel_size[i], stride[i])\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method conv1d of type object at 0x768b99e1fec0>(*(FakeTensor(..., device='cuda:0', size=(128, 128, 256)), Parameter(FakeTensor(..., device='cuda:0', size=(10752, 1, 9)))), **{'padding': FakeTensor(..., size=(), dtype=torch.int64), 'dilation': FakeTensor(..., size=(), dtype=torch.int32), 'groups': 128}):\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] list index out of range\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] from user code:\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]    File \"/tmp/ipykernel_132598/2551859191.py\", line 71, in extract_features\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     features = self.rocket(spectrogram)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py\", line 75, in forward\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     C = F.conv1d(x, self.kernels, padding=padding, dilation=dilation, groups=self.c_in)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     super().run()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1658, in CALL_FUNCTION\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, {})\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/lazy.py\", line 170, in realize_and_forward\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return getattr(self.realize(), name)(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 914, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return variables.UserFunctionVariable(fn, source=source).call_function(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1748, in CALL_FUNCTION_KW\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     tensor_variable = wrap_fx_proxy(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return _wrap_fx_proxy(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     ret_val = wrap_fake_exception(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return fn()\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(make_error_message(e)).with_traceback(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return node.target(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return self.dispatch(func, types, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return self._cached_dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     output = self._dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2354, in _dispatch_impl\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     op_impl_out = op_impl(self, func, *args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 160, in dispatch_to_op_implementations_dict\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 769, in conv\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     out = func(**kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 723, in __call__\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     return self._op(*args, **kwargs)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2325, in meta_conv\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     shape_out = calc_conv_nd_return_shape(\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2253, in calc_conv_nd_return_shape\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     _formula(dims[i], padding[i], dilation[i], kernel_size[i], stride[i])\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method conv1d of type object at 0x768b99e1fec0>(*(FakeTensor(..., device='cuda:0', size=(128, 128, 256)), Parameter(FakeTensor(..., device='cuda:0', size=(10752, 1, 9)))), **{'padding': FakeTensor(..., size=(), dtype=torch.int64), 'dilation': FakeTensor(..., size=(), dtype=torch.int32), 'groups': 128}):\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] list index out of range\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] from user code:\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]    File \"/tmp/ipykernel_132598/2551859191.py\", line 71, in extract_features\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     features = self.rocket(spectrogram)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py\", line 75, in forward\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233]     C = F.conv1d(x, self.kernels, padding=padding, dilation=dilation, groups=self.c_in)\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0419 19:43:34.563000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] WON'T CONVERT forward /workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py line 69 \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] due to: \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     super().run()\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1748, in CALL_FUNCTION_KW\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     tensor_variable = wrap_fx_proxy(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return _wrap_fx_proxy(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     ret_val = wrap_fake_exception(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return fn()\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(make_error_message(e)).with_traceback(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return node.target(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return self.dispatch(func, types, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return self._cached_dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     output = self._dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2354, in _dispatch_impl\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     op_impl_out = op_impl(self, func, *args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 160, in dispatch_to_op_implementations_dict\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 769, in conv\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     out = func(**kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 723, in __call__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return self._op(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2325, in meta_conv\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     shape_out = calc_conv_nd_return_shape(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2253, in calc_conv_nd_return_shape\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     _formula(dims[i], padding[i], dilation[i], kernel_size[i], stride[i])\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method conv1d of type object at 0x768b99e1fec0>(*(FakeTensor(..., device='cuda:0', size=(128, 128, 256)), Parameter(FakeTensor(..., device='cuda:0', size=(10752, 1, 9)))), **{'padding': FakeTensor(..., size=(), dtype=torch.int64), 'dilation': FakeTensor(..., size=(), dtype=torch.int32), 'groups': 128}):\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] list index out of range\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] from user code:\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]    File \"/workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py\", line 75, in forward\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     C = F.conv1d(x, self.kernels, padding=padding, dilation=dilation, groups=self.c_in)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     super().run()\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1748, in CALL_FUNCTION_KW\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\", line 953, in call_function\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     tensor_variable = wrap_fx_proxy(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2153, in wrap_fx_proxy\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2219, in wrap_fx_proxy_cls\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return _wrap_fx_proxy(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builder.py\", line 2315, in _wrap_fx_proxy\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2536, in get_fake_value\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2471, in get_fake_value\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     ret_val = wrap_fake_exception(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2017, in wrap_fake_exception\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return fn()\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2472, in <lambda>\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     lambda: run_node(tx.output, node, args, kwargs, nnmodule)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2604, in run_node\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     raise RuntimeError(make_error_message(e)).with_traceback(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 2586, in run_node\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return node.target(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_stats.py\", line 21, in wrapper\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1276, in __torch_dispatch__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return self.dispatch(func, types, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1816, in dispatch\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return self._cached_dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 1377, in _cached_dispatch_impl\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     output = self._dispatch_impl(func, types, args, kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_tensor.py\", line 2354, in _dispatch_impl\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     op_impl_out = op_impl(self, func, *args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 160, in dispatch_to_op_implementations_dict\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return op_implementations_dict[func](fake_mode, func, *args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_subclasses/fake_impls.py\", line 769, in conv\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     out = func(**kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 723, in __call__\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     return self._op(*args, **kwargs)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2325, in meta_conv\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     shape_out = calc_conv_nd_return_shape(\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]   File \"/usr/local/lib/python3.10/dist-packages/torch/_meta_registrations.py\", line 2253, in calc_conv_nd_return_shape\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     _formula(dims[i], padding[i], dilation[i], kernel_size[i], stride[i])\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method conv1d of type object at 0x768b99e1fec0>(*(FakeTensor(..., device='cuda:0', size=(128, 128, 256)), Parameter(FakeTensor(..., device='cuda:0', size=(10752, 1, 9)))), **{'padding': FakeTensor(..., size=(), dtype=torch.int64), 'dilation': FakeTensor(..., size=(), dtype=torch.int32), 'groups': 128}):\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] list index out of range\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] from user code:\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]    File \"/workspace/jaspar/spring_2025_dl_audio_project/MINIROCKET_Pytorch.py\", line 75, in forward\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233]     C = F.conv1d(x, self.kernels, padding=padding, dilation=dilation, groups=self.c_in)\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W0419 19:43:34.657000 132598 torch/_dynamo/convert_frame.py:1233] \n",
            "Training Batches: 0it [00:03, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m train_epoch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(train_epoch, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Start training.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m audio_files \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_G\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_G2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccompaniment_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocal_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeech_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlambda_l1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlambda_cycle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43madversarial_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvirtual_batch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 168\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, generator_2, discriminator, optimizer_D, optimizer_G, optimizer_G2, accompaniment_loader, vocal_loader, speech_loader, l1_loss, lambda_l1, lambda_cycle, adversarial_loss, device, num_epochs, virtual_batch_size, log_dir, save_audio)\u001b[0m\n\u001b[1;32m    166\u001b[0m save_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m num_epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 168\u001b[0m epoch_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_G\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_G2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccompaniment_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocal_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeech_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_l1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43madversarial_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_audio\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Loss_D:         \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_D\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
            "Cell \u001b[0;32mIn[15], line 33\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(generator, generator_2, discriminator, optimizer_D, optimizer_G, optimizer_G2, accompaniment_loader, vocal_loader, speech_loader, l1_loss, lambda_l1, lambda_cycle, adversarial_loss, device, virtual_batch_size, save_output)\u001b[0m\n\u001b[1;32m     30\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ---- batch loop ----\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ((accomp, voc), speech) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mzip\u001b[39m(accompaniment_loader, vocal_loader), speech_loader),\n\u001b[1;32m     35\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m ):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Move data to device\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     x_acc \u001b[38;5;241m=\u001b[39m accomp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)       \u001b[38;5;66;03m# [B,128,289]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     x_speech \u001b[38;5;241m=\u001b[39m speech[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)    \u001b[38;5;66;03m# [B,128,289]\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[15], line 61\u001b[0m, in \u001b[0;36mtorch_dynamo_resume_in_train_epoch_at_33\u001b[0;34m(___stack0, generator, generator_2, discriminator, optimizer_D, optimizer_G, optimizer_G2, l1_loss, lambda_cycle, adversarial_loss, device, virtual_batch_size, total_loss_D, total_loss_G, total_loss_G_adv, total_loss_cycle, grad_norms_D, grad_norms_G, num_batches)\u001b[0m\n\u001b[1;32m     58\u001b[0m pred_real \u001b[38;5;241m=\u001b[39m discriminator(voc_np, acc_np)\n\u001b[1;32m     59\u001b[0m loss_D_real \u001b[38;5;241m=\u001b[39m adversarial_loss(pred_real, real_labels)\n\u001b[0;32m---> 61\u001b[0m raw_fake \u001b[38;5;241m=\u001b[39m generator(x_in)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocal\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m fake \u001b[38;5;241m=\u001b[39m raw_fake\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     63\u001b[0m fake_crop \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnarrow(fake, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m256\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:1380\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1375\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1376\u001b[0m             )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:1164\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1162\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:547\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    544\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:986\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    984\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 986\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     put_code_state()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:715\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    713\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39minstall_callbacks())\n\u001b[1;32m    714\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[0;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py:95\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m skip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     98\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     99\u001b[0m )\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:750\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    748\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 750\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py:1361\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1358\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1359\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1361\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:231\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py:662\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 662\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    664\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:2868\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2868\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:1052\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1052\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:962\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:3048\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 3048\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py:3033\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3028\u001b[0m _step_logger()(\n\u001b[1;32m   3029\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3031\u001b[0m )\n\u001b[1;32m   3032\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 3033\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3034\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   3037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3039\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3040\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   3043\u001b[0m )\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:1136\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1133\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1135\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_replacements\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1142\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:1382\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root, replaced_outputs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1382\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:1432\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1428\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1429\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1430\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1431\u001b[0m     ):\n\u001b[0;32m-> 1432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py:1462\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1461\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1462\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1463\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py:130\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py:2340\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 2340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1863\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1859\u001b[0m     tracing_context\n\u001b[1;32m   1860\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39m_disable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[1;32m   1861\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m ):\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py:83\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 83\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:1155\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m AOTAutogradCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m   1146\u001b[0m         dispatch_and_compile,\n\u001b[1;32m   1147\u001b[0m         mod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         remote,\n\u001b[1;32m   1153\u001b[0m     )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1155\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:1131\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1129\u001b[0m functional_call \u001b[38;5;241m=\u001b[39m create_functional_call(mod, params_spec, params_len)\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39m_disable():\n\u001b[0;32m-> 1131\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:580\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    573\u001b[0m     flat_fn,\n\u001b[1;32m    574\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py:830\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[1;32m    828\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 830\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:449\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fake_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fake_mode\u001b[38;5;241m.\u001b[39mshape_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     tensorify_python_scalars(fx_g, fake_mode\u001b[38;5;241m.\u001b[39mshape_env, fake_mode)\n\u001b[0;32m--> 449\u001b[0m fw_module, bw_module \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfx_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_fwd_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inner_fwd_outputs\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# See Note [Side-Effectful Tokens in AOTAutograd]\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39munlift_effect_tokens \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    455\u001b[0m     num_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m fw_metadata\u001b[38;5;241m.\u001b[39mnum_backward_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    456\u001b[0m ):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py:1780\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.partition_fn\u001b[0;34m(gm, joint_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cuda_context:\n\u001b[1;32m   1779\u001b[0m     _recursive_joint_graph_passes(gm)\n\u001b[0;32m-> 1780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmin_cut_rematerialization_partition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/partitioners.py:1824\u001b[0m, in \u001b[0;36mmin_cut_rematerialization_partition\u001b[0;34m(joint_module, _joint_inputs, compiler, num_fwd_outputs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         memory_budget \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_budget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1823\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1824\u001b[0m saved_values \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_saved_values_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_budget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;66;03m# save_for_backward on tensors and stashes symints in autograd .ctx\u001b[39;00m\n\u001b[1;32m   1830\u001b[0m saved_sym_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(is_sym_node, saved_values))\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/partitioners.py:1494\u001b[0m, in \u001b[0;36mchoose_saved_values_set\u001b[0;34m(joint_graph, node_info, memory_budget)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m memory_budget \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node_info\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m-> 1494\u001b[0m runtime_optimized_saved_values, _ \u001b[38;5;241m=\u001b[39m \u001b[43msolve_min_cut\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoint_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_cut_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# return runtime_optimized_saved_values\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m memory_budget \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/partitioners.py:881\u001b[0m, in \u001b[0;36msolve_min_cut\u001b[0;34m(joint_graph, node_info, min_cut_options, dont_ban)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op_types\u001b[38;5;241m.\u001b[39mis_fusible(a) \u001b[38;5;129;01mand\u001b[39;00m op_types\u001b[38;5;241m.\u001b[39mis_fusible(b)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    884\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed networkx installed to perform smart recomputation \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheuristics\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    885\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/__init__.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreadwrite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Need to test with SciPy, when available\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/networkx/algorithms/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlink_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlink_prediction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlowest_common_ancestors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01misolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:672\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "# state_time = \"20250416-143134\"\n",
        "# model_dir  = \"models\"\n",
        "\n",
        "# # ----  load the raw weight dictionaries ----\n",
        "# disc_state = torch.load(f\"{model_dir}/discriminator_state_dict_{state_time}.pt\", map_location=device)\n",
        "# gen_state  = torch.load(f\"{model_dir}/generator_state_dict_{state_time}.pt\",         map_location=device)\n",
        "# gen2_state = torch.load(f\"{model_dir}/generator_2_state_dict_{state_time}.pt\",       map_location=device)\n",
        "\n",
        "# # ----   rebuild the model objects ----\n",
        "# discriminator = TsaiMiniRocketDiscriminator().to(device)\n",
        "# generator     = Waveunet(**model_config_gen).to(device)\n",
        "# generator_2   = Waveunet(**model_config_gen2).to(device)\n",
        "\n",
        "# # ---- load the weights into those models ----\n",
        "# discriminator.load_state_dict(disc_state)\n",
        "# generator.load_state_dict(gen_state)\n",
        "# generator_2.load_state_dict(gen2_state)\n",
        "\n",
        "train_epoch = torch.compile(train_epoch, mode=\"default\")\n",
        "\n",
        "\n",
        "# Start training.\n",
        "audio_files = train(\n",
        "    generator,\n",
        "    generator_2,\n",
        "    discriminator,\n",
        "    optimizer_D,\n",
        "    optimizer_G,\n",
        "    optimizer_G2,\n",
        "    accompaniment_loader,\n",
        "    vocal_loader,\n",
        "    speech_loader,\n",
        "    l1_loss,\n",
        "    train_parameters[\"lambda_l1\"],\n",
        "    train_parameters[\"lambda_cycle\"],\n",
        "    adversarial_loss,\n",
        "    device,\n",
        "    num_epochs          = train_parameters[\"num_epochs\"],\n",
        "    virtual_batch_size  = train_parameters[\"virtual_batch_size\"],\n",
        "    log_dir             = train_parameters[\"log_dir\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfH9exL5pYjM"
      },
      "outputs": [],
      "source": [
        "# display the converted audios\n",
        "print(audio_files)\n",
        "for audio in audio_files:\n",
        "    print(audio)\n",
        "    display_audio(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-zG21ZKfArk",
        "outputId": "6edd2654-1b36-4b95-9670-35d9f3d2f040"
      },
      "outputs": [],
      "source": [
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# # Start training.\n",
        "# train(\n",
        "#     generator,\n",
        "#     generator_2,\n",
        "#     discriminator,\n",
        "#     optimizer_D,\n",
        "#     optimizer_G,\n",
        "#     optimizer_G2,\n",
        "#     accompaniment_loader,\n",
        "#     vocal_loader,\n",
        "#     speech_loader,\n",
        "#     l1_loss,\n",
        "#     train_parameters[\"lambda_l1\"],\n",
        "#     train_parameters[\"lambda_cycle\"],\n",
        "#     adversarial_loss,\n",
        "#     device,\n",
        "#     num_epochs          = 50, #train_parameters[\"num_epochs\"],\n",
        "#     virtual_batch_size  = train_parameters[\"virtual_batch_size\"],\n",
        "#     log_dir             = train_parameters[\"log_dir\"],\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWffnPNExV3-"
      },
      "source": [
        "## Save the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6dDyxJmxV3-"
      },
      "outputs": [],
      "source": [
        "# assert False\n",
        "path = \"models/\"\n",
        "torch.save(generator.state_dict(), path + \"generator_state_dict_\" + now + \".pt\")\n",
        "torch.save(generator_2.state_dict(), path + \"generator_2_state_dict_\" + now + \".pt\")\n",
        "torch.save(discriminator.state_dict(), path + \"discriminator_state_dict_\" + now + \".pt\")\n",
        "\n",
        "# ------------- package everything to save -------------\n",
        "export_dict = {\n",
        "    \"train_parameters\": train_parameters,\n",
        "    \"model_config_gen\": model_config_gen,      # Waveâ€‘Uâ€‘Net (speech+accomp â†’ vocal)\n",
        "    \"model_config_gen2\": model_config_gen2,    # Waveâ€‘Uâ€‘Net (vocal â†’ speech)\n",
        "}\n",
        "import json\n",
        "\n",
        "# (optional) ensure JSONâ€‘serialisable: convert tuples â†’ lists\n",
        "def _convert(obj):\n",
        "    if isinstance(obj, tuple):\n",
        "        return list(obj)\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: _convert(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, list):\n",
        "        return [_convert(x) for x in obj]\n",
        "    return obj\n",
        "\n",
        "export_dict = _convert(export_dict)\n",
        "\n",
        "with open(f\"{path}/training_record_{now}.json\", \"w\") as fp:\n",
        "    json.dump(export_dict, fp, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJfpzB2GfAr2"
      },
      "outputs": [],
      "source": [
        "now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_dir = \"runs/\" + \"cycleGAN_experiment_\" + now\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Start training.\n",
        "train(\n",
        "    generator,\n",
        "    generator_2,\n",
        "    discriminator,\n",
        "    optimizer_D,\n",
        "    optimizer_G,\n",
        "    optimizer_G2,\n",
        "    accompaniment_loader,\n",
        "    vocal_loader,\n",
        "    speech_loader,\n",
        "    l1_loss,\n",
        "    train_parameters[\"lambda_l1\"],\n",
        "    train_parameters[\"lambda_cycle\"],\n",
        "    adversarial_loss,\n",
        "    device,\n",
        "    num_epochs          = 50, #train_parameters[\"num_epochs\"],\n",
        "    virtual_batch_size  = train_parameters[\"virtual_batch_size\"],\n",
        "    log_dir             = train_parameters[\"log_dir\"],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
