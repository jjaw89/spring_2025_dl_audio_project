{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/CreateMusdbDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oHxPG08sHd",
        "outputId": "ccf3bc4d-c3aa-4d55-c2ee-2b1add6313e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting musdb\n",
            "  Downloading musdb-0.4.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.11/dist-packages (from musdb) (2.0.2)\n",
            "Collecting stempeg>=0.2.3 (from musdb)\n",
            "  Downloading stempeg-0.2.3-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pyaml (from musdb)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from musdb) (4.67.1)\n",
            "Collecting ffmpeg-python>=0.2.0 (from stempeg>=0.2.3->musdb)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml->musdb) (6.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python>=0.2.0->stempeg>=0.2.3->musdb) (1.0.0)\n",
            "Downloading musdb-0.4.2-py2.py3-none-any.whl (13 kB)\n",
            "Downloading stempeg-0.2.3-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyaml, ffmpeg-python, stempeg, musdb\n",
            "Successfully installed ffmpeg-python-0.2.0 musdb-0.4.2 pyaml-25.1.0 stempeg-0.2.3\n",
            "Found existing installation: stempeg 0.2.3\n",
            "Uninstalling stempeg-0.2.3:\n",
            "  Successfully uninstalled stempeg-0.2.3\n",
            "/content/drive/MyDrive/DeepLearningBootcamp\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "\n",
        "# when you install musdb, pip automatically installs a version of stempeg that\n",
        "# contains a small bug. To work around this, download the stempeg folder from\n",
        "# the github to your drive.\n",
        "\n",
        "!{sys.executable} -m pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n",
        "!{sys.executable} -m pip uninstall -y stempeg    # musdb installs the wrong version of stempeg'\n",
        "\n",
        "# The path below should be changed to the location of the stempeg package in\n",
        "# your Drive\n",
        "%cd '/content/drive/MyDrive/DeepLearningBootcamp'\n",
        "\n",
        "import stempeg\n",
        "import musdb\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pA7TDxaWpaF"
      },
      "outputs": [],
      "source": [
        "############## ONLY RUN THIS CELL IF YOU NEED TO DOWNLOAD DATA #################\n",
        "#import requests\n",
        "\n",
        "#file_url = \"https://zenodo.org/records/1117372/files/musdb18.zip\"\n",
        "#zip_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18.zip\"\n",
        "#destination_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data\"\n",
        "\n",
        "#r = requests.get(file_url, stream = True)\n",
        "#with open(zip_path, \"wb\") as file:\n",
        "#  for block in r.iter_content(chunk_size = 1024):\n",
        "#    if block:\n",
        "#      file.write(block)\n",
        "\n",
        "#import zipfile\n",
        "#with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#    zip_ref.extractall(destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Gfxp1J_sLT"
      },
      "outputs": [],
      "source": [
        "class MusdbDataset(Dataset):\n",
        "\n",
        "  def __init__(self, musDB, window_size = 256, step_size = 128):\n",
        "    self.mel_specs = torch.zeros(1, 2, 128, window_size)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_songs = 0\n",
        "\n",
        "    for track in musDB:\n",
        "      stems, rate = track.stems, track.rate\n",
        "\n",
        "      num_songs += 1\n",
        "\n",
        "      # separate the vocal from other instruments and conver to mono signal\n",
        "      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n",
        "      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n",
        "\n",
        "      # compute log mel spectrogram and convert to pytorch tensor\n",
        "      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n",
        "      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n",
        "\n",
        "      start_ndx = 0\n",
        "\n",
        "      for step in range(window_size // step_size):\n",
        "        cropped_logmelspec_novocal = logmelspec_novocal[:, start_ndx:]\n",
        "        cropped_logmelspec_vocal = logmelspec_vocal[:, start_ndx:]\n",
        "        num_slices = cropped_logmelspec_novocal.shape[1] // window_size\n",
        "\n",
        "        # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "        cropped_logmelspec_novocal = cropped_logmelspec_novocal[: , 0:num_slices*window_size]\n",
        "        cropped_logmelspec_vocal = cropped_logmelspec_vocal[:, 0:num_slices*window_size]\n",
        "\n",
        "        # reshape tensors into chunks of size 128x(window_size)\n",
        "        # first dimension is number of chunks\n",
        "        cropped_logmelspec_novocal = torch.transpose(torch.reshape(cropped_logmelspec_novocal, (128, num_slices, window_size)), 0, 1)\n",
        "        cropped_logmelspec_vocal = torch.transpose(torch.reshape(cropped_logmelspec_vocal, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "        # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n",
        "        logmels = torch.cat((cropped_logmelspec_novocal.unsqueeze(1), cropped_logmelspec_vocal.unsqueeze(1)), 1)\n",
        "        logmels = self.remove_silent_layers(logmels)\n",
        "        self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n",
        "        self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "        if num_songs % 10 == 0:\n",
        "          print(str(num_songs) + \" songs processed; produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "    # remove the all zeros slice that we initialized with\n",
        "    self.mel_specs = self.mel_specs[1: , : , : , :]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n",
        "    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n",
        "\n",
        "  def cat(self, other_ds):\n",
        "    self.mel_specs = torch.cat((self.mel_specs, other_ds.mel_specs), 0)\n",
        "    self.sample_rates = torch.cat((self.sample_rates, other_ds.sample_rates), 0)\n",
        "\n",
        "  def remove_silent_layers(self, mel_specs, thresh=-30):\n",
        "    '''Removes any spectrograms from mel_specs where the vocal track is too quiet.\n",
        "    We define a chunk of audio to be 'too quiet' if the maximum value of a mel bin\n",
        "    is below the threshold. '''\n",
        "    nonzero_slices = []\n",
        "    for ndx in range(mel_specs.shape[0]):\n",
        "      if torch.max(mel_specs[ndx, 1, :, :]) >= thresh:\n",
        "        nonzero_slices.append(ndx)\n",
        "\n",
        "    return mel_specs[nonzero_slices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woM0vis0AyST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ac013d-36b1-44b2-98cf-d8cd5e5e6bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data into workspace\n"
          ]
        }
      ],
      "source": [
        "# change this string to the path where the musdb data is located\n",
        "musdb_data_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data/\"\n",
        "\n",
        "# change this string to the path where you would like to save the .pt files\n",
        "# make sure the string is in a format so that appending the file name gives\n",
        "# a valid path (i.e. be careful to include relevant slashes)\n",
        "destination_path = \"/content/drive/MyDrive/DeepLearningBootcamp/\"\n",
        "\n",
        "print(\"Loading data into workspace\")\n",
        "music_train = musdb.DB(musdb_data_path, subsets=\"train\")\n",
        "music_test = musdb.DB(musdb_data_path, subsets=\"test\")\n",
        "\n",
        "print(\"Creating MusdbDataset object\")\n",
        "musdbData_train = MusdbDataset(music_train, step_size = 128)\n",
        "musdbData_test = MusdbDataset(music_test, step_size = 128)\n",
        "\n",
        "print(\"Saving datasets as .pt files\")\n",
        "torch.save(musdbData_train, destination_path + 'musdb_withOverlap_train.pt')\n",
        "torch.save(musdbData_test, destination_path + 'musdb_withOverlap_test.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5+0Tn8cDTnY+Dk/0W16I2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}