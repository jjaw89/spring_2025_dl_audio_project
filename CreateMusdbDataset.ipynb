{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjaw89/spring_2025_dl_audio_project/blob/main/CreateMusdbDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5oHxPG08sHd",
        "outputId": "ccf3bc4d-c3aa-4d55-c2ee-2b1add6313e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "\n",
        "# when you install musdb, pip automatically installs a version of stempeg that\n",
        "# contains a small bug. To work around this, download the stempeg folder from\n",
        "# the github to your drive.\n",
        "\n",
        "# !{sys.executable} -m pip install musdb  # has some helpful data structures, also installs ffmpeg and stempeg\n",
        "# !{sys.executable} -m pip uninstall -y stempeg    # musdb installs the wrong version of stempeg'\n",
        "\n",
        "# The path below should be changed to the location of the stempeg package in\n",
        "# your Drive\n",
        "# %cd '/content/drive/MyDrive/DeepLearningBootcamp'\n",
        "\n",
        "import stempeg\n",
        "import musdb\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3pA7TDxaWpaF"
      },
      "outputs": [],
      "source": [
        "############## ONLY RUN THIS CELL IF YOU NEED TO DOWNLOAD DATA #################\n",
        "#import requests\n",
        "\n",
        "#file_url = \"https://zenodo.org/records/1117372/files/musdb18.zip\"\n",
        "#zip_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18.zip\"\n",
        "#destination_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data\"\n",
        "\n",
        "#r = requests.get(file_url, stream = True)\n",
        "#with open(zip_path, \"wb\") as file:\n",
        "#  for block in r.iter_content(chunk_size = 1024):\n",
        "#    if block:\n",
        "#      file.write(block)\n",
        "\n",
        "#import zipfile\n",
        "#with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#    zip_ref.extractall(destination_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q0Gfxp1J_sLT"
      },
      "outputs": [],
      "source": [
        "class MusdbDataset(Dataset):\n",
        "\n",
        "  def __init__(self, musDB, window_size = 256, step_size = 128):\n",
        "    self.mel_specs = torch.zeros(1, 2, 128, window_size)\n",
        "    self.sample_rates = torch.tensor([0])\n",
        "\n",
        "    num_songs = 0\n",
        "\n",
        "    for track in musDB:\n",
        "      stems, rate = track.stems, track.rate\n",
        "\n",
        "      num_songs += 1\n",
        "\n",
        "      # separate the vocal from other instruments and conver to mono signal\n",
        "      audio_novocal = librosa.to_mono(np.transpose(stems[1] + stems[2] + stems[3]))\n",
        "      audio_vocal = librosa.to_mono(np.transpose(stems[4]))\n",
        "\n",
        "      # compute log mel spectrogram and convert to pytorch tensor\n",
        "      logmelspec_novocal = torch.from_numpy(self._mel_spectrogram(audio_novocal, rate))\n",
        "      logmelspec_vocal = torch.from_numpy(self._mel_spectrogram(audio_vocal, rate))\n",
        "\n",
        "      start_ndx = 0\n",
        "\n",
        "      for step in range(window_size // step_size):\n",
        "        cropped_logmelspec_novocal = logmelspec_novocal[:, start_ndx:]\n",
        "        cropped_logmelspec_vocal = logmelspec_vocal[:, start_ndx:]\n",
        "        num_slices = cropped_logmelspec_novocal.shape[1] // window_size\n",
        "\n",
        "        # chop off the last bit so that number of stft steps is a multiple of window_size\n",
        "        cropped_logmelspec_novocal = cropped_logmelspec_novocal[: , 0:num_slices*window_size]\n",
        "        cropped_logmelspec_vocal = cropped_logmelspec_vocal[:, 0:num_slices*window_size]\n",
        "\n",
        "        # reshape tensors into chunks of size 128x(window_size)\n",
        "        # first dimension is number of chunks\n",
        "        cropped_logmelspec_novocal = torch.transpose(torch.reshape(cropped_logmelspec_novocal, (128, num_slices, window_size)), 0, 1)\n",
        "        cropped_logmelspec_vocal = torch.transpose(torch.reshape(cropped_logmelspec_vocal, (128, num_slices, window_size)), 0, 1)\n",
        "\n",
        "        # unsqueeze and concatenate these tensors. Then concatenate to the big tensor\n",
        "        logmels = torch.cat((cropped_logmelspec_novocal.unsqueeze(1), cropped_logmelspec_vocal.unsqueeze(1)), 1)\n",
        "        logmels = self.remove_silent_layers(logmels)\n",
        "        self.mel_specs = torch.cat((self.mel_specs, logmels), 0)\n",
        "        self.sample_rates = torch.cat((self.sample_rates, torch.full((num_slices,), rate)), 0)\n",
        "\n",
        "        if num_songs % 10 == 0:\n",
        "          print(str(num_songs) + \" songs processed; produced \" + str(self.mel_specs.shape[0]) + \" spectrograms\")\n",
        "\n",
        "    # remove the all zeros slice that we initialized with\n",
        "    self.mel_specs = self.mel_specs[1: , : , : , :]\n",
        "    self.sample_rates = self.sample_rates[1:]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.mel_specs.shape[0]\n",
        "\n",
        "  def __getitem__(self, ndx):\n",
        "    # returns tuple (mel spectrogram of accompaniment, mel spectrogram of vocal, rate)\n",
        "    return self.mel_specs[ndx, 0], self.mel_specs[ndx, 1], self.sample_rates[ndx]\n",
        "\n",
        "  def _mel_spectrogram(self, audio, rate):\n",
        "    # compute the log-mel-spectrogram of the audio at the given sample rate\n",
        "    return librosa.power_to_db(librosa.feature.melspectrogram(y = audio, sr = rate))\n",
        "\n",
        "  def cat(self, other_ds):\n",
        "    self.mel_specs = torch.cat((self.mel_specs, other_ds.mel_specs), 0)\n",
        "    self.sample_rates = torch.cat((self.sample_rates, other_ds.sample_rates), 0)\n",
        "\n",
        "  def remove_silent_layers(self, mel_specs, thresh=-30):\n",
        "    '''Removes any spectrograms from mel_specs where the vocal track is too quiet.\n",
        "    We define a chunk of audio to be 'too quiet' if the maximum value of a mel bin\n",
        "    is below the threshold. '''\n",
        "    nonzero_slices = []\n",
        "    for ndx in range(mel_specs.shape[0]):\n",
        "      if torch.max(mel_specs[ndx, 1, :, :]) >= thresh:\n",
        "        nonzero_slices.append(ndx)\n",
        "\n",
        "    return mel_specs[nonzero_slices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woM0vis0AyST",
        "outputId": "c9ac013d-36b1-44b2-98cf-d8cd5e5e6bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data into workspace\n",
            "Creating MusdbDataset object\n",
            "10 songs processed; produced 1171 spectrograms\n",
            "10 songs processed; produced 1309 spectrograms\n",
            "20 songs processed; produced 2452 spectrograms\n",
            "20 songs processed; produced 2503 spectrograms\n",
            "30 songs processed; produced 3738 spectrograms\n",
            "30 songs processed; produced 3789 spectrograms\n",
            "40 songs processed; produced 4855 spectrograms\n",
            "40 songs processed; produced 4917 spectrograms\n",
            "50 songs processed; produced 6097 spectrograms\n",
            "50 songs processed; produced 6149 spectrograms\n",
            "60 songs processed; produced 6886 spectrograms\n",
            "60 songs processed; produced 6925 spectrograms\n",
            "70 songs processed; produced 7357 spectrograms\n",
            "70 songs processed; produced 7407 spectrograms\n",
            "80 songs processed; produced 8637 spectrograms\n",
            "80 songs processed; produced 8713 spectrograms\n",
            "90 songs processed; produced 10116 spectrograms\n",
            "90 songs processed; produced 10177 spectrograms\n",
            "100 songs processed; produced 11347 spectrograms\n",
            "100 songs processed; produced 11419 spectrograms\n",
            "10 songs processed; produced 1187 spectrograms\n",
            "10 songs processed; produced 1259 spectrograms\n",
            "20 songs processed; produced 2362 spectrograms\n",
            "20 songs processed; produced 2451 spectrograms\n",
            "30 songs processed; produced 3554 spectrograms\n",
            "30 songs processed; produced 3611 spectrograms\n",
            "40 songs processed; produced 4824 spectrograms\n",
            "40 songs processed; produced 4907 spectrograms\n",
            "50 songs processed; produced 6153 spectrograms\n",
            "50 songs processed; produced 6223 spectrograms\n",
            "Saving datasets as .pt files\n"
          ]
        }
      ],
      "source": [
        "# change this string to the path where the musdb data is located\n",
        "# musdb_data_path = \"/content/drive/MyDrive/DeepLearningBootcamp/musdb18_data/\"\n",
        "musdb_data_path = \"/workspace/hdd_project_data/musdb18_data/\"  # for local testing\n",
        "\n",
        "\n",
        "# change this string to the path where you would like to save the .pt files\n",
        "# make sure the string is in a format so that appending the file name gives\n",
        "# a valid path (i.e. be careful to include relevant slashes)\n",
        "# destination_path = \"/content/drive/MyDrive/DeepLearningBootcamp/\"\n",
        "destination_path = \"/workspace/hdd_project_data/\"\n",
        "\n",
        "print(\"Loading data into workspace\")\n",
        "music_train = musdb.DB(musdb_data_path, subsets=\"train\")\n",
        "music_test = musdb.DB(musdb_data_path, subsets=\"test\")\n",
        "\n",
        "print(\"Creating MusdbDataset object\")\n",
        "musdbData_train = MusdbDataset(music_train, step_size = 128)\n",
        "musdbData_test = MusdbDataset(music_test, step_size = 128)\n",
        "\n",
        "print(\"Saving datasets as .pt files\")\n",
        "torch.save(musdbData_train, destination_path + 'musdb_withOverlap_train.pt')\n",
        "torch.save(musdbData_test, destination_path + 'musdb_withOverlap_test.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP5+0Tn8cDTnY+Dk/0W16I2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
